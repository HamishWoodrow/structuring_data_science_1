{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import pipeline, preprocessing\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def directory_list_generator(prime_directory):\n",
    "    \"\"\"Returns a list of all non-hidden directories\n",
    "    based on the path directory given, it will, return\n",
    "    only directories within the folder specified\"\"\"\n",
    "    directories=os.listdir(prime_directory)\n",
    "    dir_list = [x for x in directories if '.' not in x]\n",
    "    return dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_file_tabulator(dir_list):\n",
    "    \"\"\"Goes through all directories given as argument list\n",
    "    then picks up each text file and extracts text dumping it\n",
    "    to a column\"\"\"\n",
    "    paper_content = dict()\n",
    "    for txtDir in dir_list:\n",
    "        txtDir = prime_directory + txtDir\n",
    "        for txtfile in os.listdir(txtDir): #iterate through text files in directory\n",
    "            if txtfile[-3:] == 'txt':\n",
    "                document_path = txtDir + '/' + txtfile\n",
    "                with open(document_path) as fhand:\n",
    "                    content = fhand.read()\n",
    "                    paper_content[txtfile] = [content]\n",
    "    return paper_content              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_importance_pipeline(compiled_documents):\n",
    "    \"\"\"Pipeline to convert list of documents with text\n",
    "    content from papers into a sparse matrix using count\n",
    "    vectoriser.\"\"\"\n",
    "    \n",
    "    # Create numpy array of text data from input dictionary\n",
    "    text_data = []\n",
    "    text_data.append([v for k,v in compiled_documents.items()])\n",
    "    text_data = np.array(text_data[0])\n",
    "    \n",
    "    steps = [\n",
    "        ('vectorizer', CountVectorizer())\n",
    "            ]\n",
    "    reg = pipeline.Pipeline(steps)\n",
    "    ng_train_vecs = reg.fit_transform(text_data[:,0])\n",
    "    df = pd.DataFrame(ng_train_vecs.todense(), columns=[cv.get_feature_names()])\n",
    "    #id2word = dict((v, k) for k, v in ng_train_vecs.vocabulary_.items())\n",
    "    return df, ng_train_vecs.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_counter(compiled_documents):\n",
    "    \"\"\"Pipeline to convert list of documents with text\n",
    "    content from papers into a sparse matrix using count\n",
    "    vectoriser.\"\"\"\n",
    "    # Create numpy array of text data from input dictionary\n",
    "    text_data = []\n",
    "    text_data.append([v for k,v in compiled_documents.items()])\n",
    "    text_data = np.array(text_data[0])\n",
    "    # Create list of stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(['cid'])\n",
    "    # Create a CountVectorizer for parsing/counting words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=my_stop_words, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "    count_vectorizer.fit(text_data[:,0])\n",
    "    counts = count_vectorizer.transform(text_data[:,0]).transpose()\n",
    "    id2word = dict([(v, k) for k, v in count_vectorizer.vocabulary_.items()])\n",
    "    return counts, id2word,count_vectorizer,text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory to parse\n",
    "prime_directory = 'txt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_list=directory_list_generator(prime_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compiled_documents = text_file_tabulator(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts,id2word,cv,text_data = word_counter(compiled_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = matutils.Sparse2Corpus(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-16 21:39:34,487 : INFO : using symmetric alpha at 0.16666666666666666\n",
      "2017-08-16 21:39:34,489 : INFO : using symmetric eta at 0.000200762899016\n",
      "2017-08-16 21:39:34,491 : INFO : using serial LDA version on this node\n",
      "2017-08-16 21:39:34,664 : INFO : running online (multi-pass) LDA training, 6 topics, 10 passes over the supplied corpus of 3 documents, updating model once every 3 documents, evaluating perplexity every 3 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2017-08-16 21:39:35,511 : INFO : -9.774 per-word bound, 875.3 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:35,512 : INFO : PROGRESS: pass 0, at document #3/3\n",
      "2017-08-16 21:39:35,606 : INFO : topic #0 (0.167): 0.011*\"neural\" + 0.009*\"quantum\" + 0.008*\"dynamics\" + 0.008*\"network\" + 0.007*\"state\" + 0.007*\"et\" + 0.006*\"recurrence\" + 0.004*\"input\" + 0.004*\"neuron\" + 0.004*\"mean\"\n",
      "2017-08-16 21:39:35,607 : INFO : topic #3 (0.167): 0.015*\"neural\" + 0.009*\"network\" + 0.009*\"quantum\" + 0.007*\"recurrence\" + 0.007*\"dynamics\" + 0.007*\"state\" + 0.006*\"et\" + 0.005*\"learning\" + 0.004*\"energy\" + 0.003*\"neurons\"\n",
      "2017-08-16 21:39:35,608 : INFO : topic #4 (0.167): 0.010*\"neural\" + 0.009*\"quantum\" + 0.007*\"dynamics\" + 0.007*\"network\" + 0.006*\"state\" + 0.005*\"input\" + 0.004*\"recurrence\" + 0.004*\"et\" + 0.004*\"neurons\" + 0.003*\"environment\"\n",
      "2017-08-16 21:39:35,609 : INFO : topic #1 (0.167): 0.011*\"neural\" + 0.009*\"network\" + 0.009*\"quantum\" + 0.007*\"dynamics\" + 0.006*\"recurrence\" + 0.005*\"input\" + 0.004*\"state\" + 0.004*\"neurons\" + 0.004*\"environment\" + 0.003*\"et\"\n",
      "2017-08-16 21:39:35,610 : INFO : topic #2 (0.167): 0.012*\"neural\" + 0.010*\"quantum\" + 0.010*\"network\" + 0.007*\"state\" + 0.007*\"dynamics\" + 0.007*\"recurrence\" + 0.005*\"input\" + 0.004*\"et\" + 0.004*\"given\" + 0.004*\"learning\"\n",
      "2017-08-16 21:39:35,611 : INFO : topic diff=0.897657, rho=1.000000\n",
      "2017-08-16 21:39:36,294 : INFO : -8.879 per-word bound, 470.8 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:36,295 : INFO : PROGRESS: pass 1, at document #3/3\n",
      "2017-08-16 21:39:36,396 : INFO : topic #4 (0.167): 0.008*\"neural\" + 0.008*\"quantum\" + 0.005*\"dynamics\" + 0.005*\"network\" + 0.005*\"state\" + 0.004*\"input\" + 0.004*\"recurrence\" + 0.003*\"et\" + 0.003*\"neurons\" + 0.003*\"environment\"\n",
      "2017-08-16 21:39:36,397 : INFO : topic #5 (0.167): 0.011*\"neural\" + 0.006*\"quantum\" + 0.006*\"dynamics\" + 0.006*\"network\" + 0.004*\"state\" + 0.004*\"recurrence\" + 0.004*\"et\" + 0.004*\"input\" + 0.003*\"environment\" + 0.003*\"neuron\"\n",
      "2017-08-16 21:39:36,398 : INFO : topic #1 (0.167): 0.009*\"neural\" + 0.008*\"network\" + 0.008*\"quantum\" + 0.006*\"dynamics\" + 0.005*\"recurrence\" + 0.005*\"input\" + 0.004*\"state\" + 0.003*\"neurons\" + 0.003*\"environment\" + 0.003*\"et\"\n",
      "2017-08-16 21:39:36,399 : INFO : topic #0 (0.167): 0.011*\"neural\" + 0.009*\"quantum\" + 0.009*\"dynamics\" + 0.008*\"network\" + 0.007*\"et\" + 0.007*\"state\" + 0.006*\"recurrence\" + 0.004*\"input\" + 0.004*\"neuron\" + 0.004*\"mean\"\n",
      "2017-08-16 21:39:36,400 : INFO : topic #3 (0.167): 0.015*\"neural\" + 0.009*\"quantum\" + 0.009*\"network\" + 0.007*\"dynamics\" + 0.006*\"recurrence\" + 0.006*\"state\" + 0.006*\"et\" + 0.004*\"learning\" + 0.004*\"energy\" + 0.003*\"input\"\n",
      "2017-08-16 21:39:36,401 : INFO : topic diff=0.454410, rho=0.577350\n",
      "2017-08-16 21:39:37,076 : INFO : -8.569 per-word bound, 379.7 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:37,077 : INFO : PROGRESS: pass 2, at document #3/3\n",
      "2017-08-16 21:39:37,133 : INFO : topic #4 (0.167): 0.006*\"neural\" + 0.006*\"quantum\" + 0.004*\"dynamics\" + 0.004*\"network\" + 0.004*\"state\" + 0.003*\"input\" + 0.003*\"recurrence\" + 0.003*\"et\" + 0.002*\"neurons\" + 0.002*\"environment\"\n",
      "2017-08-16 21:39:37,136 : INFO : topic #2 (0.167): 0.014*\"neural\" + 0.011*\"quantum\" + 0.010*\"network\" + 0.008*\"dynamics\" + 0.007*\"state\" + 0.007*\"recurrence\" + 0.005*\"et\" + 0.005*\"input\" + 0.004*\"neurons\" + 0.004*\"learning\"\n",
      "2017-08-16 21:39:37,138 : INFO : topic #0 (0.167): 0.010*\"neural\" + 0.008*\"quantum\" + 0.008*\"dynamics\" + 0.007*\"network\" + 0.006*\"et\" + 0.006*\"state\" + 0.005*\"recurrence\" + 0.004*\"input\" + 0.004*\"neuron\" + 0.003*\"mean\"\n",
      "2017-08-16 21:39:37,139 : INFO : topic #3 (0.167): 0.013*\"neural\" + 0.008*\"quantum\" + 0.008*\"network\" + 0.006*\"dynamics\" + 0.005*\"recurrence\" + 0.005*\"state\" + 0.005*\"et\" + 0.004*\"learning\" + 0.003*\"energy\" + 0.003*\"input\"\n",
      "2017-08-16 21:39:37,141 : INFO : topic #1 (0.167): 0.007*\"neural\" + 0.006*\"network\" + 0.006*\"quantum\" + 0.005*\"dynamics\" + 0.004*\"recurrence\" + 0.004*\"input\" + 0.003*\"state\" + 0.003*\"neurons\" + 0.002*\"environment\" + 0.002*\"et\"\n",
      "2017-08-16 21:39:37,142 : INFO : topic diff=0.523460, rho=0.500000\n",
      "2017-08-16 21:39:37,971 : INFO : -8.382 per-word bound, 333.6 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:37,972 : INFO : PROGRESS: pass 3, at document #3/3\n",
      "2017-08-16 21:39:38,005 : INFO : topic #5 (0.167): 0.006*\"neural\" + 0.004*\"quantum\" + 0.003*\"dynamics\" + 0.003*\"network\" + 0.002*\"state\" + 0.002*\"recurrence\" + 0.002*\"et\" + 0.002*\"input\" + 0.002*\"environment\" + 0.002*\"neuron\"\n",
      "2017-08-16 21:39:38,006 : INFO : topic #1 (0.167): 0.005*\"neural\" + 0.004*\"network\" + 0.004*\"quantum\" + 0.004*\"dynamics\" + 0.003*\"recurrence\" + 0.003*\"input\" + 0.002*\"state\" + 0.002*\"neurons\" + 0.002*\"environment\" + 0.002*\"et\"\n",
      "2017-08-16 21:39:38,007 : INFO : topic #2 (0.167): 0.014*\"neural\" + 0.011*\"quantum\" + 0.010*\"network\" + 0.008*\"dynamics\" + 0.007*\"state\" + 0.007*\"recurrence\" + 0.005*\"et\" + 0.005*\"input\" + 0.004*\"neurons\" + 0.004*\"learning\"\n",
      "2017-08-16 21:39:38,009 : INFO : topic #3 (0.167): 0.011*\"neural\" + 0.006*\"quantum\" + 0.006*\"network\" + 0.005*\"dynamics\" + 0.005*\"recurrence\" + 0.004*\"state\" + 0.004*\"et\" + 0.003*\"learning\" + 0.003*\"energy\" + 0.002*\"input\"\n",
      "2017-08-16 21:39:38,010 : INFO : topic #4 (0.167): 0.004*\"neural\" + 0.004*\"quantum\" + 0.003*\"dynamics\" + 0.003*\"network\" + 0.003*\"state\" + 0.002*\"input\" + 0.002*\"recurrence\" + 0.002*\"et\" + 0.002*\"neurons\" + 0.002*\"environment\"\n",
      "2017-08-16 21:39:38,011 : INFO : topic diff=0.455489, rho=0.447214\n",
      "2017-08-16 21:39:38,686 : INFO : -8.288 per-word bound, 312.5 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:38,687 : INFO : PROGRESS: pass 4, at document #3/3\n",
      "2017-08-16 21:39:38,712 : INFO : topic #0 (0.167): 0.007*\"neural\" + 0.006*\"quantum\" + 0.005*\"dynamics\" + 0.005*\"network\" + 0.004*\"et\" + 0.004*\"state\" + 0.004*\"recurrence\" + 0.003*\"input\" + 0.002*\"neuron\" + 0.002*\"mean\"\n",
      "2017-08-16 21:39:38,713 : INFO : topic #4 (0.167): 0.003*\"neural\" + 0.003*\"quantum\" + 0.002*\"dynamics\" + 0.002*\"network\" + 0.002*\"state\" + 0.002*\"input\" + 0.001*\"recurrence\" + 0.001*\"et\" + 0.001*\"neurons\" + 0.001*\"environment\"\n",
      "2017-08-16 21:39:38,714 : INFO : topic #1 (0.167): 0.004*\"neural\" + 0.003*\"network\" + 0.003*\"quantum\" + 0.003*\"dynamics\" + 0.002*\"recurrence\" + 0.002*\"input\" + 0.002*\"state\" + 0.001*\"neurons\" + 0.001*\"environment\" + 0.001*\"et\"\n",
      "2017-08-16 21:39:38,715 : INFO : topic #3 (0.167): 0.009*\"neural\" + 0.005*\"quantum\" + 0.005*\"network\" + 0.004*\"dynamics\" + 0.004*\"recurrence\" + 0.003*\"state\" + 0.003*\"et\" + 0.003*\"learning\" + 0.002*\"energy\" + 0.002*\"input\"\n",
      "2017-08-16 21:39:38,717 : INFO : topic #2 (0.167): 0.014*\"neural\" + 0.010*\"quantum\" + 0.010*\"network\" + 0.008*\"dynamics\" + 0.007*\"state\" + 0.007*\"recurrence\" + 0.006*\"et\" + 0.005*\"input\" + 0.004*\"neurons\" + 0.004*\"learning\"\n",
      "2017-08-16 21:39:38,718 : INFO : topic diff=0.368947, rho=0.408248\n",
      "2017-08-16 21:39:39,307 : INFO : -8.235 per-word bound, 301.3 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:39,308 : INFO : PROGRESS: pass 5, at document #3/3\n",
      "2017-08-16 21:39:39,328 : INFO : topic #2 (0.167): 0.014*\"neural\" + 0.010*\"quantum\" + 0.010*\"network\" + 0.008*\"dynamics\" + 0.007*\"state\" + 0.007*\"recurrence\" + 0.006*\"et\" + 0.005*\"input\" + 0.004*\"neurons\" + 0.004*\"learning\"\n",
      "2017-08-16 21:39:39,329 : INFO : topic #4 (0.167): 0.002*\"neural\" + 0.002*\"quantum\" + 0.002*\"dynamics\" + 0.002*\"network\" + 0.001*\"state\" + 0.001*\"input\" + 0.001*\"recurrence\" + 0.001*\"et\" + 0.001*\"neurons\" + 0.001*\"environment\"\n",
      "2017-08-16 21:39:39,330 : INFO : topic #1 (0.167): 0.003*\"neural\" + 0.002*\"network\" + 0.002*\"quantum\" + 0.002*\"dynamics\" + 0.002*\"recurrence\" + 0.001*\"input\" + 0.001*\"state\" + 0.001*\"neurons\" + 0.001*\"environment\" + 0.001*\"et\"\n",
      "2017-08-16 21:39:39,331 : INFO : topic #3 (0.167): 0.006*\"neural\" + 0.004*\"quantum\" + 0.004*\"network\" + 0.003*\"dynamics\" + 0.003*\"recurrence\" + 0.003*\"state\" + 0.003*\"et\" + 0.002*\"learning\" + 0.002*\"energy\" + 0.001*\"input\"\n",
      "2017-08-16 21:39:39,332 : INFO : topic #0 (0.167): 0.005*\"neural\" + 0.005*\"quantum\" + 0.004*\"dynamics\" + 0.004*\"network\" + 0.003*\"et\" + 0.003*\"state\" + 0.003*\"recurrence\" + 0.002*\"input\" + 0.002*\"neuron\" + 0.002*\"mean\"\n",
      "2017-08-16 21:39:39,333 : INFO : topic diff=0.284621, rho=0.377964\n",
      "2017-08-16 21:39:39,933 : INFO : -8.205 per-word bound, 295.0 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:39,934 : INFO : PROGRESS: pass 6, at document #3/3\n",
      "2017-08-16 21:39:39,952 : INFO : topic #1 (0.167): 0.002*\"neural\" + 0.002*\"network\" + 0.002*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"recurrence\" + 0.001*\"input\" + 0.001*\"state\" + 0.001*\"neurons\" + 0.001*\"environment\" + 0.001*\"et\"\n",
      "2017-08-16 21:39:39,954 : INFO : topic #5 (0.167): 0.002*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.001*\"state\" + 0.001*\"recurrence\" + 0.001*\"et\" + 0.001*\"input\" + 0.001*\"environment\" + 0.001*\"neuron\"\n",
      "2017-08-16 21:39:39,955 : INFO : topic #4 (0.167): 0.002*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.001*\"state\" + 0.001*\"input\" + 0.001*\"recurrence\" + 0.001*\"et\" + 0.001*\"neurons\" + 0.001*\"environment\"\n",
      "2017-08-16 21:39:39,956 : INFO : topic #2 (0.167): 0.014*\"neural\" + 0.010*\"quantum\" + 0.010*\"network\" + 0.008*\"dynamics\" + 0.007*\"state\" + 0.007*\"recurrence\" + 0.006*\"et\" + 0.005*\"input\" + 0.004*\"neurons\" + 0.004*\"learning\"\n",
      "2017-08-16 21:39:39,957 : INFO : topic #3 (0.167): 0.005*\"neural\" + 0.003*\"quantum\" + 0.003*\"network\" + 0.002*\"dynamics\" + 0.002*\"recurrence\" + 0.002*\"state\" + 0.002*\"et\" + 0.001*\"learning\" + 0.001*\"energy\" + 0.001*\"input\"\n",
      "2017-08-16 21:39:39,959 : INFO : topic diff=0.213606, rho=0.353553\n",
      "2017-08-16 21:39:40,536 : INFO : -8.187 per-word bound, 291.5 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:40,537 : INFO : PROGRESS: pass 7, at document #3/3\n",
      "2017-08-16 21:39:40,553 : INFO : topic #5 (0.167): 0.002*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.001*\"state\" + 0.001*\"recurrence\" + 0.001*\"et\" + 0.001*\"input\" + 0.001*\"environment\" + 0.001*\"neuron\"\n",
      "2017-08-16 21:39:40,554 : INFO : topic #0 (0.167): 0.003*\"neural\" + 0.003*\"quantum\" + 0.003*\"dynamics\" + 0.002*\"network\" + 0.002*\"et\" + 0.002*\"state\" + 0.002*\"recurrence\" + 0.001*\"input\" + 0.001*\"neuron\" + 0.001*\"mean\"\n",
      "2017-08-16 21:39:40,555 : INFO : topic #3 (0.167): 0.004*\"neural\" + 0.002*\"quantum\" + 0.002*\"network\" + 0.002*\"dynamics\" + 0.002*\"recurrence\" + 0.002*\"state\" + 0.002*\"et\" + 0.001*\"learning\" + 0.001*\"energy\" + 0.001*\"input\"\n",
      "2017-08-16 21:39:40,557 : INFO : topic #1 (0.167): 0.001*\"neural\" + 0.001*\"network\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"recurrence\" + 0.001*\"input\" + 0.001*\"state\" + 0.001*\"neurons\" + 0.001*\"environment\" + 0.001*\"et\"\n",
      "2017-08-16 21:39:40,558 : INFO : topic #4 (0.167): 0.001*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.001*\"state\" + 0.001*\"input\" + 0.001*\"recurrence\" + 0.001*\"et\" + 0.001*\"neurons\" + 0.001*\"environment\"\n",
      "2017-08-16 21:39:40,559 : INFO : topic diff=0.158121, rho=0.333333\n",
      "2017-08-16 21:39:41,134 : INFO : -8.177 per-word bound, 289.5 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:41,134 : INFO : PROGRESS: pass 8, at document #3/3\n",
      "2017-08-16 21:39:41,152 : INFO : topic #4 (0.167): 0.001*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.001*\"state\" + 0.001*\"input\" + 0.000*\"recurrence\" + 0.000*\"et\" + 0.000*\"neurons\" + 0.000*\"environment\"\n",
      "2017-08-16 21:39:41,153 : INFO : topic #2 (0.167): 0.014*\"neural\" + 0.010*\"quantum\" + 0.010*\"network\" + 0.008*\"dynamics\" + 0.007*\"state\" + 0.007*\"recurrence\" + 0.006*\"et\" + 0.005*\"input\" + 0.004*\"neurons\" + 0.004*\"learning\"\n",
      "2017-08-16 21:39:41,154 : INFO : topic #0 (0.167): 0.002*\"neural\" + 0.002*\"quantum\" + 0.002*\"dynamics\" + 0.002*\"network\" + 0.001*\"et\" + 0.001*\"state\" + 0.001*\"recurrence\" + 0.001*\"input\" + 0.001*\"neuron\" + 0.001*\"mean\"\n",
      "2017-08-16 21:39:41,155 : INFO : topic #3 (0.167): 0.003*\"neural\" + 0.002*\"quantum\" + 0.002*\"network\" + 0.001*\"dynamics\" + 0.001*\"recurrence\" + 0.001*\"state\" + 0.001*\"et\" + 0.001*\"learning\" + 0.001*\"energy\" + 0.001*\"input\"\n",
      "2017-08-16 21:39:41,156 : INFO : topic #5 (0.167): 0.001*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.001*\"state\" + 0.001*\"recurrence\" + 0.001*\"et\" + 0.000*\"input\" + 0.000*\"environment\" + 0.000*\"neuron\"\n",
      "2017-08-16 21:39:41,158 : INFO : topic diff=0.116421, rho=0.316228\n",
      "2017-08-16 21:39:41,742 : INFO : -8.171 per-word bound, 288.3 perplexity estimate based on a held-out corpus of 3 documents with 30345 words\n",
      "2017-08-16 21:39:41,742 : INFO : PROGRESS: pass 9, at document #3/3\n",
      "2017-08-16 21:39:41,759 : INFO : topic #1 (0.167): 0.001*\"neural\" + 0.001*\"network\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"recurrence\" + 0.000*\"input\" + 0.000*\"state\" + 0.000*\"neurons\" + 0.000*\"environment\" + 0.000*\"et\"\n",
      "2017-08-16 21:39:41,760 : INFO : topic #3 (0.167): 0.002*\"neural\" + 0.001*\"quantum\" + 0.001*\"network\" + 0.001*\"dynamics\" + 0.001*\"recurrence\" + 0.001*\"state\" + 0.001*\"et\" + 0.001*\"learning\" + 0.001*\"energy\" + 0.001*\"input\"\n",
      "2017-08-16 21:39:41,761 : INFO : topic #2 (0.167): 0.014*\"neural\" + 0.010*\"quantum\" + 0.010*\"network\" + 0.008*\"dynamics\" + 0.007*\"state\" + 0.007*\"recurrence\" + 0.006*\"et\" + 0.005*\"input\" + 0.004*\"neurons\" + 0.004*\"learning\"\n",
      "2017-08-16 21:39:41,762 : INFO : topic #4 (0.167): 0.001*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.000*\"state\" + 0.000*\"input\" + 0.000*\"recurrence\" + 0.000*\"et\" + 0.000*\"neurons\" + 0.000*\"environment\"\n",
      "2017-08-16 21:39:41,763 : INFO : topic #0 (0.167): 0.002*\"neural\" + 0.002*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.001*\"et\" + 0.001*\"state\" + 0.001*\"recurrence\" + 0.001*\"input\" + 0.001*\"neuron\" + 0.001*\"mean\"\n",
      "2017-08-16 21:39:41,764 : INFO : topic diff=0.085674, rho=0.301511\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=6, id2word=id2word, passes=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-16 21:39:41,771 : INFO : topic #0 (0.167): 0.002*\"neural\" + 0.002*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.001*\"et\" + 0.001*\"state\" + 0.001*\"recurrence\" + 0.001*\"input\" + 0.001*\"neuron\" + 0.001*\"mean\" + 0.001*\"lines\" + 0.001*\"neurons\" + 0.001*\"layer\" + 0.001*\"backpropagation\" + 0.001*\"structure\" + 0.001*\"quasiperiodic\" + 0.001*\"energy\" + 0.001*\"problem\" + 0.001*\"dynamical\" + 0.001*\"environment\"\n",
      "2017-08-16 21:39:41,772 : INFO : topic #1 (0.167): 0.001*\"neural\" + 0.001*\"network\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"recurrence\" + 0.000*\"input\" + 0.000*\"state\" + 0.000*\"neurons\" + 0.000*\"environment\" + 0.000*\"et\" + 0.000*\"problem\" + 0.000*\"energy\" + 0.000*\"diagonal\" + 0.000*\"backpropagation\" + 0.000*\"fk\" + 0.000*\"given\" + 0.000*\"case\" + 0.000*\"mean\" + 0.000*\"learning\" + 0.000*\"layer\"\n",
      "2017-08-16 21:39:41,774 : INFO : topic #2 (0.167): 0.014*\"neural\" + 0.010*\"quantum\" + 0.010*\"network\" + 0.008*\"dynamics\" + 0.007*\"state\" + 0.007*\"recurrence\" + 0.006*\"et\" + 0.005*\"input\" + 0.004*\"neurons\" + 0.004*\"learning\" + 0.004*\"mean\" + 0.004*\"neuron\" + 0.004*\"environment\" + 0.004*\"energy\" + 0.003*\"lines\" + 0.003*\"given\" + 0.003*\"neural network\" + 0.003*\"backpropagation\" + 0.003*\"fk\" + 0.003*\"layer\"\n",
      "2017-08-16 21:39:41,775 : INFO : topic #3 (0.167): 0.002*\"neural\" + 0.001*\"quantum\" + 0.001*\"network\" + 0.001*\"dynamics\" + 0.001*\"recurrence\" + 0.001*\"state\" + 0.001*\"et\" + 0.001*\"learning\" + 0.001*\"energy\" + 0.001*\"input\" + 0.001*\"neurons\" + 0.001*\"neuron\" + 0.001*\"fk\" + 0.001*\"mean\" + 0.001*\"patterns\" + 0.001*\"lines\" + 0.001*\"layer\" + 0.000*\"embedding\" + 0.000*\"backpropagation\" + 0.000*\"quasiperiodic\"\n",
      "2017-08-16 21:39:41,777 : INFO : topic #4 (0.167): 0.001*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.000*\"state\" + 0.000*\"input\" + 0.000*\"recurrence\" + 0.000*\"et\" + 0.000*\"neurons\" + 0.000*\"environment\" + 0.000*\"mean\" + 0.000*\"lines\" + 0.000*\"neuron\" + 0.000*\"sig\" + 0.000*\"quasiperiodic\" + 0.000*\"backpropagation\" + 0.000*\"networks\" + 0.000*\"energy\" + 0.000*\"neural network\" + 0.000*\"standard\"\n",
      "2017-08-16 21:39:41,778 : INFO : topic #5 (0.167): 0.001*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.000*\"state\" + 0.000*\"recurrence\" + 0.000*\"et\" + 0.000*\"input\" + 0.000*\"environment\" + 0.000*\"neuron\" + 0.000*\"learning\" + 0.000*\"mean\" + 0.000*\"neural network\" + 0.000*\"energy\" + 0.000*\"sig\" + 0.000*\"layer\" + 0.000*\"feedforward\" + 0.000*\"lines\" + 0.000*\"form\" + 0.000*\"quasiperiodic\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"neural\" + 0.002*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.001*\"et\" + 0.001*\"state\" + 0.001*\"recurrence\" + 0.001*\"input\" + 0.001*\"neuron\" + 0.001*\"mean\" + 0.001*\"lines\" + 0.001*\"neurons\" + 0.001*\"layer\" + 0.001*\"backpropagation\" + 0.001*\"structure\" + 0.001*\"quasiperiodic\" + 0.001*\"energy\" + 0.001*\"problem\" + 0.001*\"dynamical\" + 0.001*\"environment\"'),\n",
       " (1,\n",
       "  '0.001*\"neural\" + 0.001*\"network\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"recurrence\" + 0.000*\"input\" + 0.000*\"state\" + 0.000*\"neurons\" + 0.000*\"environment\" + 0.000*\"et\" + 0.000*\"problem\" + 0.000*\"energy\" + 0.000*\"diagonal\" + 0.000*\"backpropagation\" + 0.000*\"fk\" + 0.000*\"given\" + 0.000*\"case\" + 0.000*\"mean\" + 0.000*\"learning\" + 0.000*\"layer\"'),\n",
       " (2,\n",
       "  '0.014*\"neural\" + 0.010*\"quantum\" + 0.010*\"network\" + 0.008*\"dynamics\" + 0.007*\"state\" + 0.007*\"recurrence\" + 0.006*\"et\" + 0.005*\"input\" + 0.004*\"neurons\" + 0.004*\"learning\" + 0.004*\"mean\" + 0.004*\"neuron\" + 0.004*\"environment\" + 0.004*\"energy\" + 0.003*\"lines\" + 0.003*\"given\" + 0.003*\"neural network\" + 0.003*\"backpropagation\" + 0.003*\"fk\" + 0.003*\"layer\"'),\n",
       " (3,\n",
       "  '0.002*\"neural\" + 0.001*\"quantum\" + 0.001*\"network\" + 0.001*\"dynamics\" + 0.001*\"recurrence\" + 0.001*\"state\" + 0.001*\"et\" + 0.001*\"learning\" + 0.001*\"energy\" + 0.001*\"input\" + 0.001*\"neurons\" + 0.001*\"neuron\" + 0.001*\"fk\" + 0.001*\"mean\" + 0.001*\"patterns\" + 0.001*\"lines\" + 0.001*\"layer\" + 0.000*\"embedding\" + 0.000*\"backpropagation\" + 0.000*\"quasiperiodic\"'),\n",
       " (4,\n",
       "  '0.001*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.000*\"state\" + 0.000*\"input\" + 0.000*\"recurrence\" + 0.000*\"et\" + 0.000*\"neurons\" + 0.000*\"environment\" + 0.000*\"mean\" + 0.000*\"lines\" + 0.000*\"neuron\" + 0.000*\"sig\" + 0.000*\"quasiperiodic\" + 0.000*\"backpropagation\" + 0.000*\"networks\" + 0.000*\"energy\" + 0.000*\"neural network\" + 0.000*\"standard\"'),\n",
       " (5,\n",
       "  '0.001*\"neural\" + 0.001*\"quantum\" + 0.001*\"dynamics\" + 0.001*\"network\" + 0.000*\"state\" + 0.000*\"recurrence\" + 0.000*\"et\" + 0.000*\"input\" + 0.000*\"environment\" + 0.000*\"neuron\" + 0.000*\"learning\" + 0.000*\"mean\" + 0.000*\"neural network\" + 0.000*\"energy\" + 0.000*\"sig\" + 0.000*\"layer\" + 0.000*\"feedforward\" + 0.000*\"lines\" + 0.000*\"form\" + 0.000*\"quasiperiodic\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "from gensim.corpora import Dictionary, MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-16 21:39:59,227 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-08-16 21:39:59,230 : INFO : built Dictionary(1 unique tokens: ['6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n2\\n2\\n\\n \\n\\n \\n \\n]\\nE\\nN\\n.\\ns\\nc\\n[\\n \\n \\n\\n1\\nv\\n5\\n3\\n9\\n6\\n0\\n\\n.\\n\\n9\\n0\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nQuantum Neural Machine Learning -\\n\\nBackpropagation and Dynamics\\n\\nCarlos Pedro Gonçalves\\n\\nSeptember 23, 2016\\n\\nUniversity of Lisbon, Institute of Social and Political Sciences,\\n\\ncgoncalves@iscsp.ulisboa.pt\\n\\nAbstract\\n\\nThe current work addresses quantum machine learning in the con-\\ntext of Quantum Artiﬁcial Neural Networks such that the networks’\\nprocessing is divided in two stages: the learning stage, where the net-\\nwork converges to a speciﬁc quantum circuit, and the backpropaga-\\ntion stage where the network eﬀectively works as a self-programing\\nquantum computing system that selects the quantum circuits to solve\\ncomputing problems. The results are extended to general architectures\\nincluding recurrent networks that interact with an environment, cou-\\npling with it in the neural links’ activation order, and self-organizing\\nin a dynamical regime that intermixes patterns of dynamical stochas-\\nticity and persistent quasiperiodic dynamics, making emerge a form of\\nnoise resilient dynamical record.\\n\\nKeywords: Quantum Artiﬁcial Neural Networks, Machine Learn-\\n\\ning, Open Quantum Systems, Complex Quantum Systems\\n\\n1\\n\\n\\x0c1 Introduction\\n\\nQuantum Artiﬁcial Neural Networks (QuANNs) provide an approach to\\nquantum machine learning based on networked quantum computation (Chris-\\nley, 1995; Kak, 1995; Menneer and Narayanan, 1995; Behrman et al., 1996;\\nMenneer, 1998; Ivancevic and Ivancevic, 2010; Schuld et al., 2014a; Schuld\\net al., 2014b; Gonçalves, 2015a, 2015b).\\n\\nIn the current work, we address two major building blocks for quantum\\nneural machine learning: feedforward dynamics and quantum backpropaga-\\ntion, introduced as a quantum circuit selection control dynamics that intro-\\nduces a feeding back of the neural network, thus, after propagating quantum\\ninformation in the feedforward direction, during the quantum learning stage,\\nquantum information is, then, propagated backwards so that the network\\neﬀectively functions as a self-programming quantum computing system, eﬃ-\\nciently solving computational problems.\\n\\nThe concept of quantum neural backpropagation with which we work\\nis diﬀerent from the classical ANNs’ error backpropagation1. The quan-\\ntum backpropagation dynamics is integrated in a two stage neural cognition\\nscheme: there is a feedforward learning stage such that the output neurons’\\nstates, initially separable from the input neurons’ states, converge during a\\nneural processing time (cid:52)to to correlated states with the input layer, and then\\nthere is a backpropagation stage, where the output neurons act as a control\\nsystem that triggers diﬀerent quantum circuits that are implemented on the\\ninput neurons, conditionally transforming their state in such a way that a\\ngiven computational problem is solved.\\n\\nThe approach to quantum machine learning that we assume here is, there-\\nfore, worked from a notion of measurement-based quantum machine learn-\\n\\n1Even though the quantum backpropagation that we work with ends up implementing\\na form of quantum adaptive error correction, in the sense that, for a feedforward network,\\nthe input layer is conditionally transformed so that it exhibits the ﬁring patterns that\\nsolve a given computational problem.\\n\\n2\\n\\n\\x0cing2, where the learning stage corresponds to a quantum measurement dy-\\nnamics, in which the system records the state of the target, in order to later\\nuse that record for solving some task that involves the conditional transfor-\\nmation of the target’s state, conditional, in this case, on the computational\\nrecord.\\n\\nIn the present work, we ﬁrst show (section 2) how this approach to quan-\\ntum machine learning can be integrated, within a supervised learning set-\\nting, in feedforward neural networks, to solve computational problems. We,\\nthus, begin by introducing an Hamiltonian framework for quantum neural\\nmachine learning with basic feedforward neural networks (subsection 2.1),\\nintegrating quantum measurement theory and dividing the quantum neu-\\nral dynamics in the learning stage and the backpropagation stage, we then\\napply the framework to two example problems: the ﬁring pattern selection\\nproblem (addressed in subsection 2.2.), where the neural network places the\\ninput layer in a speciﬁc well-deﬁned ﬁring conﬁguration, from an initially ar-\\nbitrary superposition of neural ﬁring patterns, the n-to-m Boolean functions’\\nrepresentation problem (addressed in subsection 2.3), where the goal for the\\nnetwork is to correct the input layer so that it represents an arbitrary n-to-m\\nBoolean function. The ﬁrst problem is solved with a network size equal to\\n2m (where m is the size of the input layer), the second problem is solved for\\na network size of n + 2m.\\n\\nIn section 3, the results from section 2 are expanded to more general\\narchitectures that can be represented by any ﬁnite digraph (subsection 3.1)\\ndealing with an unsupervised learning framework, where the network’s neural\\nprocessing is comprised of feedforward computations and backpropagation\\n\\n2To learn, from the Proto-Germanic *liznojan, synthesizing the sense of following or\\nﬁnding the track, from the Proto-Indo-European *leis- (track, furrow). It is also important\\nto consider the Latin term praehendere: to capture, to grasp, to record; prae (in front of )\\nand hendere, connected with hedera (ivy) a plant that grabs on to things. In the quantum\\nmeasurement setting, the measurement apparatus interacts with the target system in such\\na way that the measurement apparatus’ state converges to a correlated state with the\\ntarget, eﬀectively recording the target with respect to some observable.\\n\\n3\\n\\n\\x0cdynamics that close recurrent loops. We address how these networks compute\\nan environment in terms of the iterated activation of the network, such that\\nthe computation is conditional on the neural links’ activation order.\\n\\nSection 3’s computational framework is, therefore, that of open systems\\nquantum computation with changing orders of gates. The changing orders\\nof gates comes from Aharonov et al.’s (1990) original work on superpositions\\nof time evolutions of quantum systems, and has received recent attention\\nregarding the possibility of quantum computation with greater computing\\npower than the ﬁxed quantum circuit model (Procopio, et al., 2015; Brukner,\\n2014; Chiribella, et al., 2013). The main advantage of this approach is that\\nit allows the research on how a QuANN may process an environment without\\ngiving it a speciﬁc ﬁnal state goal that may direct its computation, thus, the\\nQuANN behaves as an (artiﬁcial) complex adaptive system that responds to\\nthe environment solely based on its networked architecture and the initial\\nstate of the environment plus network. In this case, the way in which the\\nnetwork responds to the environment must be analyzed at the level of the\\ndiﬀerent emergent dynamics for the network’s quantum averages.\\n\\nIn subsection 3.2, we analyze the mean total neural ﬁring energy’s emer-\\ngent dynamics, for an example of a recurrent neural network, showing that\\nthe computation of the environment by the network makes emerge complex\\nneural dynamics that combine elements of regularity, in the form of persistent\\nquasiperiodic recurrences, and elements of emergent dynamical stochasticity\\n(a form of emergent neural noise), the presence of both elements at the level\\nof the mean total neural ﬁring energy shares dynamical signatures akin to the\\nedge of chaos dynamics found in classical cellular automata and nonlinear\\ndynamical systems (Packard, 1988; Crutchﬁeld and Young, 1990; Langton,\\n1990; Wolfram, 2002), random Boolean networks (Kauﬀman and Johnsen,\\n1991; Kauﬀman, 1993) and classical neural networks (Gorodkin et al., 1993;\\nBertschinger and Natschläger, 2004).\\n\\nThe quasiperiodic recurrences constitute a form of “noise” resilient dy-\\n\\n4\\n\\n\\x0cnamical record. We also ﬁnd, in the simulations, patterns that are closer to\\na noisy chaotic regime, as well as stronger resilient quasiperiodic patterns\\nwith toroidal attractors that show up in the mean energy dynamics.\\n\\nIn section 4, a ﬁnal reﬂection is provided on the article’s main results\\nincluding the relation of section 3’s results and research on classical neural\\nnetworks.\\n\\n2 Quantum Neural Machine Learning\\n\\n2.1 Learning and Backpropagation in Feedforward Net-\\n\\nworks\\n\\nIn classical ANNs, a neuron with a binary ﬁring activity can be described in\\nterms a binary alphabet A2 = {0, 1}, with 0 representing a nonﬁring neural\\nstate and 1 representing a ﬁring neural state. For QuANNs, on the other\\nhand, the neuron’s quantum neural states are described by a two-dimensional\\nHilbert Space H2, spanned by the computational basis B2 = {|0(cid:105) ,|1(cid:105)}, where\\n|0(cid:105) encodes a nonﬁring neural state and |1(cid:105) encodes a ﬁring neural state.\\nThese states have a physical description as the eigenstates of a neural ﬁring\\nHamiltonian:\\n\\nwhere τ is measured seconds, so that the corresponding neural ﬁring fre-\\nquency given by (1/τ )Hz, and ˆσ3 is Pauli’s operator:\\n\\n(1)\\n\\n(2)\\n\\n(cid:32)ˆ1 − ˆσ3\\n\\n(cid:33)\\n\\nˆH =\\n\\n\\x02\\n\\n2π\\nτ\\n\\n2\\n\\n(cid:32)\\n\\n(cid:33)\\n\\n1\\n0\\n0 −1\\n\\nˆσ3 = |0(cid:105)(cid:104)0| − |1(cid:105)(cid:104)1| =\\n\\n5\\n\\n\\x0cThe computational basis B2, then, satisﬁes the eigenvalue equation:\\n\\nˆH |r(cid:105) =\\n\\n\\x02r |r(cid:105)\\n\\n2π\\nτ\\n\\n(3)\\n\\nwith r = 0, 1. Thus, the nonﬁring state corresponds to an energy eigenstate\\nof zero Joules and the ﬁring state corresponds to an energy eigenstate of\\n\\x022π/τ Joules. In the special case where the neural ﬁring frequency is such\\nthat the following condition holds:\\n\\n\\x02 = 1J\\n\\n2π\\nτ\\n\\n(4)\\n\\nthen, the nonﬁring energy eigenvalue is zero Joules (0J) and the ﬁring eigen-\\nvalue is one Joule (1J). In this special case, the numbers associated to the ket\\nvector notation |0(cid:105) and |1(cid:105), which usually take the role of logical values (bits)\\nin standard quantum computation, coincide exactly with the energy eigen-\\nvalues of the quantum artiﬁcial neuron. The three Pauli operators’ actions\\non the neuron’s ﬁring energy eigenstates are given, respectively, by:\\n\\nˆσ1 |r(cid:105) = |1 − r(cid:105)\\n\\nˆσ2 |r(cid:105) = i(−1)r |1 − r(cid:105)\\n\\nˆσ3 |r(cid:105) = (−1)r |r(cid:105)\\n\\n(5)\\n\\n(6)\\n\\n(7)\\n\\n(8)\\n\\n(9)\\n\\n(cid:33)\\n\\n(cid:33)\\n\\n(cid:32)\\n(cid:32)\\n\\n0 1\\n1 0\\n\\n0 −i\\n0\\ni\\n\\nwith ˆσ3 described by Eq.(2) and ˆσ1, ˆσ2 deﬁned as:\\n\\nˆσ1 = |0(cid:105)(cid:104)1| + |1(cid:105)(cid:104)0| =\\n\\nˆσ2 = −i|0(cid:105)(cid:104)1| + i|1(cid:105)(cid:104)0| =\\n\\n6\\n\\n\\x0c2\\n\\n2\\n\\n2\\n\\n(cid:9), where AN\\n\\n2 = (cid:8)|r(cid:105) : r ∈ AN\\n\\nA neural network with N neurons has, thus, an associated Hilbert space,\\ngiven by the N-tensor product of copies of H2: H⊗N\\n, which is spanned\\nby the basis B⊗N\\nis the set of all length N\\n2 = {r1r2...rN : rk ∈ A2, k = 1, 2, ..., N}. The basis B⊗N\\nbinary strings: AN\\ncorresponds to the set of well-deﬁned ﬁring patterns for the neural network,\\nwhich coincide with the classical states of a corresponding classical ANN, the\\ngeneral state of the quantum network can, however, exhibit a superposition\\nof neural ﬁring patterns described by a normalized ket vector, in the space\\nH⊗N\\n\\n, deﬁned as:\\n\\n2\\n\\n2\\n\\n(cid:88)\\n\\n|ψ(cid:105) =\\n\\nψ(r)|r(cid:105)\\n\\nwith the normalization condition:(cid:88)\\n\\nr∈AN\\n\\n2\\n\\nr∈AN\\n\\n2\\n\\n|ψ(r)|2 = 1\\n\\n(10)\\n\\n(11)\\n\\nFor such an N neuron network we can introduce the local operators for\\n\\nk = 1, 2, ..., N:\\n\\nˆHk = ˆ1⊗(k−1) ⊗ ˆH ⊗ ˆ1⊗(N−k)\\n\\n(12)\\nwith ˆH1 = ˆH ⊗ ˆ1⊗(N−1) and ˆHN = ˆ1⊗(N−1) ⊗ ˆH, where ˆH has the structure\\ndeﬁned in Eq.(1) and ˆ1 = |0(cid:105)(cid:104)0| + |1(cid:105)(cid:104)1| is the unit operator on H2. The\\nnetwork’s total Hamiltonian ˆHN et is, thus, given by the sum:\\n\\nˆHN et =\\n\\nˆHk\\n\\n(13)\\n\\nwhich yields the Hamiltonian for the total neural ﬁring energy, satisfying the\\nequation:\\n\\n(cid:33)\\n\\nˆHN et |r1r2...rN(cid:105) =\\n\\n\\x02rk\\n\\n2π\\nτ\\n\\n|r1r2...rN(cid:105)\\n\\n(14)\\n\\nN(cid:88)\\n\\nk=1\\n\\n(cid:32) N(cid:88)\\n\\nk=1\\n\\n7\\n\\n\\x0cAn elementary example of a QuANN is the two-layer feedforward net-\\nwork composed of a system of m input neurons and n output neurons. The\\noutput neurons are transformed conditionally on the input neurons’ states,\\nso that the neural network has an associated neural links’ operator with the\\nstructure:\\n\\n(cid:88)\\n\\nr∈Am\\n\\n2\\n\\n|r(cid:105)(cid:104)r| n(cid:79)\\n\\nk=1\\n\\nˆL(cid:52)t =\\n\\ne− i\\x02(cid:52)t ˆHk,r\\n\\n(15)\\n\\n3(cid:88)\\n\\nj=1\\n\\nwhere (cid:52)t is a neural processing period and the conditional Hamiltonians ˆHk,r\\nare operators on H2 with the general structure given by:\\n\\nˆHk,r = −\\x02\\n\\n2\\n\\nωk(r)\\n(cid:52)to\\n\\nˆ1 +\\n\\nθk(r)\\n(cid:52)to\\n\\nuj,k(r)\\n\\n\\x02\\n2\\n\\nˆσj\\n\\n(16)\\n\\nsuch that the angles ωk(r) and θk(r) are measured in radians and (cid:52)to is a\\nlearning period measured in seconds (the time interval (cid:52)to will play here\\na role analogous to the inverse of the learning rate of classical ANNs), the\\nuj,k(r) terms are the components of a real unit vector ˆuk(r) and ˆσj are Pauli’s\\noperators. Thus, the conditional unitary evolution for each output neuron’s\\nstate, expressed by the neural links’ operator, is given by the conditional\\nU(2) transformations:\\n\\ne− i\\x02(cid:52)t ˆHk,r = ei\\n\\nωk (r)(cid:52)t\\n2(cid:52)to ˆUˆuk(r)\\n\\nwith the rotation operators deﬁned as:\\n\\n(cid:19)\\n\\n(cid:18) θk(r)(cid:52)t\\n\\n2(cid:52)to\\n\\n= cos\\n\\nˆ1 − i sin\\n\\nˆUˆuk(r)\\n\\n(cid:18) θk(r)(cid:52)t\\n\\n2(cid:52)to\\n\\n(cid:52)to\\n\\n(cid:21)\\n(cid:20)θk(r)(cid:52)t\\n(cid:20)θk(r)(cid:52)t\\n(cid:19) 3(cid:88)\\n\\n(cid:52)to\\n\\nuj,k(r)ˆσj\\n\\nj=1\\n\\n(cid:21)\\n\\n=\\n\\n(17)\\n\\n(18)\\n\\nwhere the phase transform angles ωk(r), the rotation angles θk(r) and the\\n\\n8\\n\\n\\x0cunit vectors ˆuk(r) can be diﬀerent for diﬀerent output neurons, so that each\\noutput neuron’s state is transformed conditionally on the input layer’s neu-\\nrons’ ﬁring patterns. Depending on the Hamiltonian parameters, we can have\\na full connection, where the parameters’ values are diﬀerent for each diﬀer-\\nent input layer’s ﬁring pattern, or local connections, where the Hamiltonian\\nparameters only depend on some of the input neurons’ ﬁring patterns.\\n\\nThe operator ˆL(cid:52)t is, thus, given by:\\n\\nˆL(cid:52)t =\\n\\n(cid:88)\\n|r(cid:105)(cid:104)r| n(cid:79)\\n\\nei\\n\\nr∈Am\\n\\n2\\n\\nk=1\\n\\n|r(cid:105)(cid:104)r| n(cid:79)\\n\\nk=1\\n\\nωk (r)(cid:52)t\\n2(cid:52)to ˆUˆuk(r)\\n\\n(cid:88)\\n\\nr∈Am\\n\\n2\\n\\n=\\n\\ne− i\\x02(cid:52)t ˆHk,r =\\n\\n(cid:20)θk(r)(cid:52)t\\n\\n(cid:21)\\n\\n(cid:52)to\\n\\n(19)\\n\\nFor (cid:52)t → (cid:52)to, the unitary evolution operators described by Eqs.(17) and\\n(18) converge to the result:\\n\\n(cid:34)\\n\\ncos\\n\\n(cid:18)θk(r)\\n\\n(cid:19)\\n\\n2\\n\\nˆ1 − i sin\\n\\n= ei\\n\\nωk (r)\\n\\n2\\n\\ne− i\\x02(cid:52)to ˆHk,r =\\n2 ˆUˆuk(r) [θk(r)] =\\n\\n(cid:35)\\n\\nωk (r)\\n\\n= ei\\n\\n(cid:18) θk(r)\\n\\n(cid:19) 3(cid:88)\\n\\n2\\n\\nj=1\\n\\nuj,k(r)ˆσj\\n\\n(20)\\n\\nAssuming, now, an initial state for the neural network given by the general\\n\\nstructure:\\n\\n|ψ0(cid:105) =\\n\\n(cid:88)\\n\\nr∈Am\\n\\n2\\n\\nψ0(r)|r(cid:105) n(cid:79)\\n\\nk=1\\n\\n|φk(cid:105)\\n\\n(21)\\n\\nwith |φk(cid:105) = φk(0)|0(cid:105) + φk(1)|1(cid:105), then, the state after a neural processing\\n\\n9\\n\\n\\x0cperiod of (cid:52)t is given by:\\n\\n(cid:88)\\n\\nr∈Am\\n\\n2\\n\\n=\\n\\nψ0(r)|r(cid:105) n(cid:79)\\n\\n|ψ(cid:52)t(cid:105) = ˆL(cid:52)t |ψ0(cid:105) =\\ne− i\\x02(cid:52)t ˆHk,r |φk(cid:105)\\n\\nk=1\\n\\n(22)\\n\\nFrom, Eq.(20), as (cid:52)t → (cid:52)to the neural network’s state converges to:\\n\\n(cid:88)\\n\\nr∈Am\\n\\n2\\n\\n=\\n\\nψ0(r)|r(cid:105) n(cid:79)\\n\\n|ψ(cid:52)to(cid:105) =\\n2 ˆUˆuk(r) [θk(r)]|φk(cid:105)\\n\\nωk (r)\\n\\nei\\n\\nk=1\\n\\n(23)\\n\\nso that each ouput neuron’s state undergoes a parametrized U(2) transfor-\\nmation that is conditional on the input neurons’ ﬁring patterns.\\n\\nA speciﬁc framework for the neural state transition, during the learning\\nperiod, can be implemented, assuming the state for each output neuron at\\nthe beginning of the learning period to be given by:\\n\\n|φk(cid:105) = |+(cid:105) =\\n\\n|0(cid:105) + |1(cid:105)√\\n\\n2\\n\\n(24)\\n\\nIn the context of supervised learning, a computational problem with expres-\\nsion in terms of binary ﬁring patterns can be addressed, as illustrated in the\\n2 → A2, so that\\nnext subsections, by introducing functions of the form fk : Am\\nthe Hamiltonian parameters are given by:\\n\\nωk(r) = (1 − fk(r)) π\\n\\n2 − fk(r)\\n\\n2\\n\\n, fk(r),\\n\\n(cid:19)\\n\\nπ\\n\\n1 − fk(r)√\\n\\n2\\n\\nθk(r) =\\n\\n(cid:18)1 − fk(r)√\\n\\n2\\n\\nˆuk(r) =\\n\\n(25)\\n\\n(26)\\n\\n(27)\\n\\n10\\n\\n\\x0cthen, the state of the neural network converges to the ﬁnal result:\\n\\n(cid:88)\\n\\nr∈Am\\n\\n2\\n\\nψ0(r)|r(cid:105) n(cid:79)\\n\\nk=1\\n\\n|ψ(cid:52)to(cid:105) =\\n\\n|fk(r)(cid:105)\\n\\n(28)\\n\\nthis means that the ouput neurons, which are, at the beginning of the neural\\nlearning period, in an equally-weighted symmetric superposition of ﬁring\\nand nonﬁring states (separable from the input neurons’ states and from each\\nother), tend, as (cid:52)t → (cid:52)to, to a correlated state, such that each neuron ﬁres\\nfor the branches |r(cid:105) in which fk(r) = 1 and does not ﬁre for the branches\\nin which fk(r) = 0. The lower the learning period (cid:52)to is, the faster the\\nconvergence takes place, which means that the time interval (cid:52)to plays a role\\nakin to the inverse of the learning rate in classical neural networks.\\n\\nNow, the concept of backpropagation we work with, as stated previously,\\ninvolves transforming the input neurons’ state conditionally on the output\\nneurons’ state so that a certain computational task is solved, this means\\nthat the feedforward network behaves as a quantum computer, deﬁned as\\na system of quantum registers, which uses the output layer’s neurons (the\\noutput registers) to select the appropriate quantum circuits to be applied to\\nthe input layer’s neurons (input registers). The backpropagation operator ˆB\\nallows for this quantum computational scheme, so that we have:\\n\\nˆB =\\n\\nˆCs ⊗ |s(cid:105)(cid:104)s|\\n\\n(29)\\n\\n(cid:88)\\n\\ns∈An\\n\\n2\\n\\n2\\n\\nwhere each ˆCs corresponds to a diﬀerent quantum circuit deﬁned on the\\ninput neurons’ Hilbert space H⊗m\\n. Thus, the backpropagation dynamics\\nmeans that the neural network will implement diﬀerent quantum circuits on\\nthe input layer depending on the ﬁring patterns of the output layer. Instead\\nof being restricted to a single quantum algorithm, the neural network is thus\\nable to implement diﬀerent quantum algorithms, taking advantage of a form\\nof quantum parallel computation, where the output neurons assume the role\\n\\n11\\n\\n\\x0cof an internal control system for a quantum circuit selection dynamics.\\n\\nWith this framework, the whole feedforward neural network functions as a\\nform of self-programming quantum computer with a two-stage computation:\\nthe ﬁrst stage is the neural learning stage, where the neural links’ operator is\\napplied, the second stage is the backpropagation, where the backpropagation\\noperator is applied, leading to the state transition rule:\\n\\n|ψ0(cid:105) → ˆB ˆL(cid:52)to |ψ0(cid:105)\\n\\n(30)\\n\\nSince, instead of a single algorithm, the network conditionally applies\\ndiﬀerent algorithms, depending upon the result of the learning stage, there\\ntakes place a form of (parallel) quantum computationally-based adaptive cog-\\nnition, such that the cognitive system (the network) selects the appropriate\\nalgorithm to be applied, in order to eﬃciently solve a given computational\\nproblem.\\n\\nIn the case of Eq.(28), applying the general form of the backpropagation\\n\\noperator (Eq.(29)) leads to:\\n\\n(cid:88)\\n\\nr∈Am\\n\\n2\\n\\nψ0(r) ˆCf1(r)f2(r)...fn(r) |r(cid:105) n(cid:79)\\n\\nˆB ˆL(cid:52)to |ψ0(cid:105)\\n|fk(r)(cid:105)\\n\\nk=1\\n\\n(31)\\n\\nfor the output neurons(cid:78)n\\n\\nwhere f1(r)f2(r)...fn(r) is the n-bit string that results from the concatenation\\nof the outputs of the functions fk(r), with k = 1, 2, ..., n. In this last case, for\\neach input layer’s ﬁring pattern |r(cid:105), there is a corresponding ﬁring pattern\\nk=1 |fk(r)(cid:105), resulting from the learning stage which\\ntriggers a corresponding quantum circuit to be applied to the input layer in\\nthe backpropagation stage.\\n\\nWhile the operator ˆB can have a general structure, the examples of most\\ninterest, in terms of networked quantum computation, come from the cases in\\nwhich the operator ˆB has the form of a neural links’ operator, thus, quantum\\n\\n12\\n\\n\\x0cinformation can propagate backwards from the output layer to the input layer\\ntransforming the input layer by following the neural connections, so that we\\nget:\\n\\n(cid:33)\\n\\nˆB =\\n\\ne− i\\x02(cid:52)t ˆHk,s\\n\\n⊗ |s(cid:105)(cid:104)s|\\n\\n(32)\\n\\n(cid:32) m(cid:79)\\n\\n(cid:88)\\n\\ns∈An\\n\\n2\\n\\nk=1\\n\\nIn this later case, one is dealing with recurrent QuANNs. We will return to\\nthese types of networks in section 3. We now apply the above approach to\\ntwo computational problems.\\n\\n2.2 Firing Pattern Selection\\n\\nThe ﬁring pattern selection problem for a two-layer feedforward network is\\nsuch that given m input neurons, at the end of the backpropagation stage, the\\ninput neurons always exhibit a speciﬁc ﬁring pattern, to solve this problem\\nwe need the output layer to also have m neurons. The network’s state at the\\nbeginning of the neural processing is assumed to be of the form:\\n\\n\\uf8eb\\uf8ed(cid:88)\\n\\nr∈Am\\n\\n2\\n\\n\\uf8f6\\uf8f8 ⊗ |+(cid:105)⊗m\\n\\n|ψ0(cid:105) =\\n\\nψ0(r)|r(cid:105)\\n\\n(33)\\n\\nGiven two m length Boolean strings r and q, let rk and qk denote, respec-\\ntively, the k-th symbol in r and q, then, let fk,q be an m-to-one parametrized\\nBoolean function deﬁned such that:\\n\\nfk,q(r) = rk ⊕ qk\\n\\n(34)\\n\\nthus, fk,q always takes the k-th symbol in the string r and the k-th symbol in\\nthe string q yielding the value of 1 if they are diﬀerent and 0 if they coincide.\\nUsing the previous section’s framework, the Hamiltonian parameters are\\n\\ndeﬁned as:\\n\\nωk(r) = (1 − fk,q(r)) π\\n\\n(35)\\n\\n13\\n\\n\\x0c2 − fk,q(r)\\n\\nθk(r) =\\n\\n2\\n1 − fk,q(r)\\n\\n√\\n\\nπ\\n\\n2\\n\\nu1(r) = u3(r) =\\n\\nu2(r) = fk,q(r)\\n\\nwith k = 1, 2, ..., m. As (cid:52)t → (cid:52)to, we get:\\n\\n(cid:20)\\n\\nπ\\n\\ncos\\n\\n(cid:18)2 − fk,q(r)\\n\\ne− i\\x02(cid:52)to ˆHk,r =\\nˆ1−\\n\\n(cid:19)\\n\\nπ\\n\\n4\\n\\n(cid:17)(cid:21)\\n\\nπ\\n\\n(1 − fk,q(r)) ˆW + fk,q(r)ˆσ2\\n\\n1−fk,q(r)\\n\\n(cid:18)2 − fk,q(r)\\n\\n= ei\\n\\n2\\n\\n(cid:19)(cid:16)\\n\\n−i sin\\n\\n4\\n\\n(36)\\n\\n(37)\\n\\n(38)\\n\\n(39)\\n\\nwhere ˆW is the Walsh-Haddamard transform (ˆσ1 + ˆσ3) /2.\\n\\nThus, the learning stage, with (cid:52)t → (cid:52)to, leads to the quantum state\\n\\ntransition for the neural network:\\n\\nˆL(cid:52)to |ψ0(cid:105) =\\n\\ne− i\\x02(cid:52)to ˆHk,r |+(cid:105)\\n\\n(cid:88)\\n\\nr∈Am\\n\\n2\\n\\n=\\n\\nψ0(r)|r(cid:105) m(cid:79)\\n(cid:88)\\n\\nψ0(r)|r(cid:105) m(cid:79)\\n\\nk=1\\n\\nr∈Am\\n\\n2\\n\\nk=1\\n\\n|rk ⊕ qk(cid:105)\\n\\n(40)\\n\\nThis means that the k-th output neuron ﬁres when the k-th input neuron’s\\nﬁring pattern diﬀers from qk (when the input neuron is in the wrong state)\\nand does not ﬁre otherwise, so that the neuron eﬀectively identiﬁes an error\\nin corresponding input neuron. The backpropagation operator is deﬁned as:\\n\\n(cid:88)\\n\\ns∈Am\\n\\n2\\n\\n(cid:32) m(cid:79)\\n\\n(cid:88)\\n\\ns∈Am\\n\\n2\\n\\nk=1\\n\\n(cid:2)(1 − sk) ˆ1 + sk ˆσ1\\n\\n(cid:3)(cid:33)\\n\\nˆB =\\n\\nˆCs ⊗ |s(cid:105)(cid:104)s| =\\n\\n⊗ |s(cid:105)(cid:104)s|\\n\\n(41)\\n\\nwhere sk is the k-th symbol in the binary string s.\\n\\nIn quantum computation terms, Eq.(41) is structured around controlled\\n\\n14\\n\\n\\x0cnegations (CNOT gates), such that if the k-th output neuron is ﬁring then the\\nNOT gate (which has the form of Pauli’s operator ˆσ1) will be applied to the\\ncorresponding input neuron, otherwise the input neuron will stay unchanged,\\nthus, for each alternative ﬁring pattern of the output neurons, a diﬀerent\\nquantum circuit is applied, comprised of the tensor product of unit gates\\nand NOT gates. After the learning and backpropagation stages, the ﬁnal\\nstate of the neural network is, then, given by:\\n\\nˆB ˆL(cid:52)to |ψ0(cid:105) = |q(cid:105) ⊗\\n\\nψ0(r)\\n\\n|rk ⊕ qk(cid:105)\\n\\n(42)\\n\\n\\uf8eb\\uf8ed(cid:88)\\n\\nr∈Am\\n\\n2\\n\\nm(cid:79)\\n\\nk=1\\n\\n\\uf8f6\\uf8f8\\n\\nthat is, the input layer’s state exhibits the ﬁring pattern |q(cid:105), while the ouput\\nneurons’ state is described by the superposition:\\n\\n(cid:88)\\n\\nm(cid:79)\\n\\n|χ(cid:105) =\\n\\nψ0(r)\\n\\n|rk ⊕ qk(cid:105)\\n\\n(43)\\n\\n2\\n\\nk=1\\n\\nr∈Am\\n\\nwhere the sum is over each ﬁring pattern state(cid:78)m\\n\\nk=1 |rk ⊕ qk(cid:105) which records\\nwhether or not the corresponding input neurons’ states had to be trans-\\nformed to lead to the well-deﬁned ﬁring pattern |q(cid:105). The QuANN, thus,\\nchanges each alternative ﬁring pattern of the input layer so that it always\\nexhibits a speciﬁc ﬁring pattern from an arbitrary initial superposition of\\nﬁring patterns. The ﬁring pattern selection problem is thus solved in two\\nsteps (the two stages) with a network of size 2m. The solution to the ﬁring\\npattern selection problem can be incorporated in the solution to the n-to-m\\nBoolean functions’ representation as we show next.\\n\\n2.3 Representation of n-to-m Boolean Functions\\n\\nWhile, in the ﬁring pattern selection problem, the goal was for the network to\\nplace the input layer in a well-deﬁned ﬁring pattern, the goal for the Boolean\\n\\n15\\n\\n\\x0cfunctions’ representation problem is to place it in an equally weighted su-\\nperposition of ﬁring patterns that represent all the alternative sequences of\\nan n to m Boolean function, where the ﬁrst n input neurons correspond to\\nthe input string for the Boolean function and the remaining m input neu-\\nrons correspond to the function’s output string. Again we have a conditional\\ncorrection of the input layer so that it represents a speciﬁc quantum state\\nsolving a computational problem.\\n\\nLet, then, g : An\\n\\n2 be a Boolean function. For h ∈ An\\ng(h)k to be the the k-th symbol in the Boolean string g(h) ∈ Am\\ndenote the concatenation of two strings h ∈ An\\nbe an (n + m)-to-one parametrized Boolean function deﬁned as follows:\\n\\n2, we deﬁne\\n2 , we also\\n2 as hr, then, let fk\\n\\n2, r ∈ Am\\n\\n2 → Am\\n\\nfk(hr) = rk ⊕ g(h)k\\n\\n(44)\\n\\nConsidering, now, a two-layer feedforward network with n + m input neu-\\nrons and m output neurons, and setting again the Hamiltonian parameters,\\nsuch that, instead of the Boolean function applied in Eqs.(35) to (38) we now\\nuse fk(hr), then, we obtain the unitary operators for (cid:52)t → (cid:52)to:\\ne− i\\x02(cid:52)to ˆHk,hr =\\nˆ1−\\n\\n(cid:18)2 − fk(hr)\\n\\n(cid:19)\\n\\n1−fk (hr)\\n\\n= ei\\n\\n(cid:20)\\n\\nπ\\n\\nπ\\n\\ncos\\n\\n4\\n\\n(cid:17)(cid:21)\\n\\n(cid:18)2 − fk(hr)\\n\\n2\\n\\n(cid:19)(cid:16)\\n\\n4\\n\\n−i sin\\n\\nπ\\n\\n(1 − fk(hr)) ˆW + fk(hr)ˆσ2\\n\\nwith k = 1, 2, ..., m. Let us, now, consider an initial state for the neural\\nnetwork given by:\\n\\n|ψ0(cid:105) = |ψinput(cid:105) ⊗ |+(cid:105)⊗m\\n\\nwith the input layer’s state |ψinput(cid:105) deﬁned by the tensor product:\\n\\n|ψinput(cid:105) = |+(cid:105)⊗n ⊗ |+(cid:105)⊗m\\n\\n16\\n\\n(45)\\n\\n(46)\\n\\n(47)\\n\\n\\x0cThe state transition for the learning stage, then, yields:\\n\\nˆL(cid:52)to |ψ0(cid:105) =\\n\\n(cid:88)\\n\\nh∈An\\n\\n2\\n\\n=\\n\\n\\uf8eb\\uf8ed(cid:88)\\n2 |r(cid:105) m(cid:79)\\n\\uf8eb\\uf8ed(cid:88)\\n\\n2− m\\n\\n2− m\\n\\nr∈Am\\n\\n2\\n\\nr∈Am\\n\\n2\\n\\nk=1\\n\\n2 |r(cid:105) m(cid:79)\\n\\nk=1\\n\\n2− n\\n\\n2 |h(cid:105) ⊗\\n\\n(cid:88)\\n\\nh∈An\\n\\n2\\n\\n2− n\\n\\n2 |h(cid:105) ⊗\\n\\n|rk ⊕ g(h)k(cid:105)\\n\\ne− i\\x02(cid:52)to ˆHk,hr |+(cid:105)\\n\\n\\uf8f6\\uf8f8 =\\n\\uf8f6\\uf8f8 (48)\\n\\nThe backpropagation operator is now deﬁned as:\\n\\n(cid:88)\\n\\ns∈Am\\n\\n2\\n\\nˆB =\\n\\nˆCs ⊗ |s(cid:105)(cid:104)s| =\\n\\n(cid:88)\\n\\ns∈Am\\n\\n2\\n\\n(cid:2)(1 − sk) ˆ1 + sk ˆσ1\\n\\nm(cid:79)\\n\\nk=1\\n\\n(cid:3)(cid:33)\\n\\n⊗ |s(cid:105)(cid:104)s|\\n\\n(49)\\n\\nagain with sk being the k-th symbol in the binary string s.\\n\\nThe ﬁnal state, after neural learning and backpropagation, is given by:\\n\\n(cid:32)\\n\\nˆ1⊗n\\n\\n\\uf8eb\\uf8ed(cid:88)\\n\\nh∈An\\n\\n2\\n\\nˆB ˆL(cid:52)to |ψ0(cid:105) =\\n\\n2− n\\n\\n2 |hg(h)(cid:105)\\n\\n(50)\\n\\n\\uf8f6\\uf8f8 ⊗ |+(cid:105)⊗m\\n\\nso that the input layer represents the Boolean function g and the output\\nlayer remains in its initial state |+(cid:105)⊗m. The general Boolean function repre-\\nsentation problem is, thus, solved in two steps, with a neural network size of\\nn + 2m.\\n\\nWhile the present section’s examples show the implementation of QuANNs\\nto solve computational problems, QuANNs can also be used to implement a\\nform of adaptive cognition of an environment where the network functions as\\nan open quantum networked computing system. We now explore this later\\ntype of application of QuANNs connecting it to networks with general archi-\\ntectures and to an approach to quantum computation where the ordering of\\nquantum gates is not ﬁxed (Procopio, et al., 2015; Brukner, 2014; Chiribella,\\n\\n17\\n\\n\\x0cet al., 2013; Aharonov, et al., 1990).\\n\\n3 General Architectures and Quantum Neural\\n\\nCognition\\n\\nThe previous section addressed the solution of computational problems by\\nfeedforward QuANNs with backpropagation. In the current section, instead\\nof a ﬁxed layered structure, the connectivity of the network can be described\\nby any ﬁnite digraph. For these networks, the feedforward and the back-\\npropagation resurface as basic building blocks for more complex dynamics.\\nNamely, the feedforward neural computation takes place at the local neuron\\nlevel connections, and the backpropagation occurs whenever recurrence is\\npresent, that is, whenever the network has closed cycles.\\n\\nThe main problem addressed, in the present section, is the network’s cog-\\nnition of an environment taken as a target system and processed iteratively\\nby the network such that, at each iteration, the network does not have a\\nﬁxed activation order but, instead, is conditionally transformed on the en-\\nvironment’s eigenstates in terms of diﬀerent neural activation orders, also,\\ninstead of a ﬁnal state, encoding a certain neural ﬁring pattern, the network’s\\nprocessing of the environment must be addressed in terms of the emergent\\ndynamics at the level of the quantum averages.\\n\\n3.1 General Architecture Networks\\nLet us consider a neuron collection N = {N1, N2, ..., Nn}, and deﬁne a general\\ndigraph G for neural connections between neurons such that if (Nj, Nk) ∈ G,\\nthen Nj takes the role of an input neuron and Nk the role of the output\\nneuron, we deﬁne for each neuron Nk ∈ N its set of input neurons under G as\\nNk = {Nj : (Nj, Nk) ∈ G}, then, we can consider the subset of N composed\\nof the neurons that receive input links from other neurons, that is N0 =\\n\\n18\\n\\n\\x0c{Nk : Nk (cid:54)= Ø, k = 1, 2, ..., n}. Using these deﬁnitions we can introduce the\\nneural links’ operator set L, comprised of the neural links’ operators for each\\nneuron that receives, under G, input neural links from other neurons:\\n\\n(cid:110) ˆLk : Nk ∈ N0\\n\\n(cid:111)\\n\\nL =\\n\\n(51)\\n\\n(cid:88)\\n\\nwith the neural links’ operators ˆLk deﬁned as operators on the Hilbert space\\nH⊗n\\n2 with the general structure (Gonçalves, 2015b):\\n\\nˆLk =\\n\\n|s(cid:105)(cid:104)s| ⊗ Lk(sin) ⊗ |s(cid:48)(cid:105)(cid:104)s(cid:48)|\\n\\n(52)\\n\\ns∈Ak−1\\n\\n2\\n\\n,s(cid:48)∈An−k\\n\\n2\\n\\nwhere sin is a substring, taken from the binary word ss(cid:48), that matches in\\nss(cid:48) the activation pattern of the input neurons for the k-th neuron, under\\nthe neural network’s architecture, in the same order and binary sequence\\nas it appears in ss(cid:48), Lk(sin) is a neural links’ function that maps the input\\nsubstring sin to a U(2) operator on the two-dimensional Hilbert space H2,\\nthus, the k-th neuron is transformed conditionally on the ﬁring patterns of\\nits input neurons under G. This means that the network has a feedforward\\nexpression at each neuron level.\\n\\nThe architecture of a QuANN satisfying the above conditions is thus given\\n\\nA =(cid:0)N ,G,H⊗n\\n2 ,L(cid:1)\\n\\nby the structure:\\n\\n(53)\\nNow, considering the set of indices I = {k : Nk ∈ N0}, if we deﬁne the\\nnatural ordering of indices k1, k2, ..., k#I, such that ki < kj for i < j, then,\\nwe can deﬁne a general neural network operator as a product of the form:\\n\\nˆLΠ = ˆLΠ(k#I )... ˆLΠ(k2)\\n\\nˆLΠ(k1)\\n\\n(54)\\n\\nwhere Π is a permutation operation on the indices k1, k2, ..., k#I. There are,\\nthus, #I! alternative permutations. Of these alternative permutations some\\n\\n19\\n\\n\\x0cmay coincide up to a global phase factor, which leads to the same ﬁnal state\\nfor the network up to a global phase factor.\\n\\nWe can, thus, deﬁne a set LN et of neural network operators ˆLΠ such that\\nfor there is no pair of operators ˆLΠ and ˆLΠ(cid:48) ∈ LN et, with Π (cid:54)= Π(cid:48), that\\ncoincides up to a global phase factor. The cardinality of any such set LN et\\ntherefore, always satisﬁes the inequality #LN et ≤ #I!.\\n\\nFor a given operator ˆLΠ, the sequence of feedorward transformations (lo-\\ncal neural activations) is ﬁxed, the backpropagation occurs in the form of\\nrecurrence whenever there is a a closed loop, so that information eventually\\nfeeds back to a neuron.\\n\\nNow, given a basis for an environment, taken as a target system to be\\n\\nprocessed by the neural network:\\n\\nBE = {|ε1(cid:105) ,|ε2(cid:105) , ...,|εm(cid:105)}\\n\\n(55)\\n\\nwith m = #LN et, spanning the Hilbert space HE, the neural processing of\\nthe environment by the network is deﬁned by the operator on the combined\\nspace HE+N et = HE ⊗ H⊗n\\n2 :\\n\\nˆUN et =\\n\\n|εk(cid:105)(cid:104)εk| ⊗ FN et(k)\\n\\n(56)\\n\\nm(cid:88)\\n\\nk=1\\n\\nwhere FN et is a bijection from {1, 2, ..., m} onto LN et. Assuming an initial\\nstate of the network plus environment to be described by a density operator\\non the space HE+N et, with the general form:\\n\\nm(cid:88)\\n\\n|εk(cid:105)(cid:104)εk(cid:48)| ⊗ (cid:88)\\n\\nˆρE+N et(0) =\\n\\nk,k(cid:48)=1\\n\\nr,r(cid:48)∈An\\n\\n2\\n\\nρk,k(cid:48),r,r(cid:48)(0)|r(cid:105)(cid:104)r(cid:48)|\\n\\n(57)\\n\\nThe state transition for the environment plus neural network, is, thus,\\n\\n20\\n\\n\\x0cgiven by the rule:\\n\\nm(cid:88)\\n\\nk,k(cid:48)=1\\n\\n=\\n\\n|εk(cid:105)(cid:104)εk(cid:48)| ⊗\\n\\n\\uf8eb\\uf8ed (cid:88)\\n\\nr,r(cid:48)∈An\\n\\n2\\n\\nˆUN et ˆρE+N et(0) ˆU\\n\\nρk,k(cid:48),r,r(cid:48)(0)FN et(k)|r(cid:105)(cid:104)r(cid:48)| FN et(k(cid:48))†\\n\\n(58)\\n\\n†\\nN et =\\n\\n\\uf8f6\\uf8f8\\n\\nThe above results allow for an iterative scheme for the neural state tran-\\nsition. Assuming, for the above structure, a repeated (iterated) activation\\nof the neural network in its interaction with the environment, we obtain a\\nsequence of density operators ˆρE+N et(0), ˆρE+N et(1), ..., ˆρE+N et(l), .... Expand-\\ning the general density operator at the step l − 1 as:\\n\\nm(cid:88)\\n\\nk,k(cid:48)=1\\n\\n=\\n\\n|εk(cid:105)(cid:104)εk(cid:48)| ⊗\\n\\n\\uf8eb\\uf8ed (cid:88)\\n\\nr,r(cid:48)∈An\\n\\n2\\n\\nˆρE+N et(l − 1) =\\n\\nρk,k(cid:48),r,r(cid:48)(l − 1)|r(cid:105)(cid:104)r(cid:48)|\\n\\n\\uf8f6\\uf8f8\\n\\n(59)\\n\\nthe dynamical rule for the network’s state transition is, thus, given by:\\n\\nm(cid:88)\\n\\nk,k(cid:48)=1\\n\\n=\\n\\n|εk(cid:105)(cid:104)εk(cid:48)| ⊗\\n\\n\\uf8eb\\uf8ed (cid:88)\\n\\nr,r(cid:48)∈An\\n\\n2\\n\\nˆρE+N et(l) = ˆUN et ˆρE+N et(l − 1) ˆU\\n\\n†\\nN et =\\n\\nρk,k(cid:48),r,r(cid:48)(l − 1)FN et(k)|r(cid:105)(cid:104)r(cid:48)| FN et(k(cid:48))†\\n\\n\\uf8f6\\uf8f8 (60)\\n\\nUsing Eq.(13), the iterative scheme for the neural processing of the envi-\\nronment leads to a sequence of values for the mean total neural ﬁring energy:\\n\\n(cid:68) ˆHN et\\n\\n(cid:69)\\n\\n(cid:16)\\n\\n= Tr\\n\\nn(cid:88)\\n\\nl\\n\\n=\\n\\n(cid:16)\\n\\nˆρE+N et(l)ˆ1E ⊗ ˆHN et\\nˆρE+N et(l)ˆ1E ⊗ ˆHj\\n\\nTr\\n\\nj=1\\n\\n=\\n\\n(cid:17)\\n(cid:17)\\n(cid:68) ˆHj\\n\\nn(cid:88)\\n\\n=\\n\\n=\\n\\n(cid:69)\\n\\nl\\n\\n(61)\\n\\nj=1\\n\\n21\\n\\n\\x0cwhere ˆ1E =(cid:80)m\\n\\nl\\n\\nk=1 |εk(cid:105)(cid:104)εk| is the unit operator on the environment’s Hilbert\\nspace HE. The emergent neural dynamics that results from the network’s\\ncomputation of the environment can, thus, be analyzed in terms of the se-\\nquence of means\\n\\n(cid:68) ˆHN et\\n\\n(cid:69)\\n\\n.\\n\\nAs shown in Gonçalves (2015b), the iteration of QuANNs has a correspon-\\ndence with nonlinear dynamical maps at the level of the quantum means for\\nHermitian operators that can be represented, in the neural ﬁring basis, as a\\nsum of projectors on those basis vectors. This implies that some of the tools\\nfrom nonlinear dynamics can be imported to the analysis of quantum neural\\nnetworks with respect to the relevant quantum averages. Namely, in regards\\nto the sequences of means\\n, we have a real-valued time series and\\ncan applying delay embedding techniques to address, statistically, the main\\ngeometric and topological properties of the newtork’s mean energy dynamics.\\nFor a lag3 of h, T iterations of the neural network and an embedding\\ndimension of dE, setting ξ = (dE − 1)h we can obtain, from the original\\nseries of means, an ordered sequence of points in dE-dimensional Euclidean\\nspace RdE:\\n\\n(cid:68) ˆHN et\\n\\n(cid:69)\\n\\nl\\n\\n(cid:18)(cid:68) ˆHN et\\n\\n(cid:69)\\n\\n(cid:68) ˆHN et\\n\\n(cid:69)\\n\\n(cid:68) ˆHN et\\n\\n(cid:69)\\n\\n(cid:19)\\n\\n(62)\\n\\nxu =\\n\\n,\\n\\nu+ξ\\n\\n, ...,\\n\\nu+ξ−h\\n\\nu+ξ−(dE−1)h\\n\\nwith u = 1, 2, ..., TdE = T − (dE − 1)h. Given the embedded sequence xu, we\\ncan take advantage of the Euclidean space metric topology and calculate the\\ndistance matrix for each pair of values:\\n\\n(63)\\nwhere (cid:107).(cid:107) is the Euclidean norm. Since the matrix is symmetric, all the\\nrelevant information is present in either one of the two halves divided by the\\n\\nDu,u(cid:48) = (cid:107)xu − xu(cid:48)(cid:107)\\n\\n3A criterion for the deﬁtion of the lag, in the context of time series’ delay embedding,\\ncan be set in terms the ﬁrst zero crossing of the series autocorrelation function (Nayfeh\\nand Balachandran, 2004).\\n\\n22\\n\\n\\x0cmain diagonal, considering one of these halves, we have Td = TdE −1 diagonal\\nlines parallel to the main diagonal corresponding to the distances between\\npoints θ periods away from each other, for θ = 1, 2, ..., Td, the number of\\nembedded points is, in turn, TdE, which means that the number of points in\\nthe parallel diagonal lines is (T 2\\ndE\\n\\n− TdE )/2.\\n\\nIf the sequence of embedded points is periodic with period θ, then, all\\ndiagonals corresponding to the periods θ(cid:48) = b · θ, with b = 1, 2, ..., have zero\\ndistance, therefore the we get xu+bθ = xu, which leads to the condition for\\nthe mean energy:\\n\\n(cid:68) ˆHN et\\n\\n(cid:69)\\n\\n(cid:68) ˆHN et\\n\\n(cid:69)\\n\\nu+bθ+ξ−th\\n\\n=\\n\\nu+ξ−th\\n\\n(64)\\n\\nfor b = 1, 2, ... and t = 0, 1, ..., dE − 1. This condition is not met for emergent\\naperiodic dynamics.\\n\\nThe analysis of the embedded dynamics can be introduced by using the\\nEuclidean space metric topology and working with the open δ-neighborhoods,\\nthus, for each period (each diagonal) θ = 1, 2, ..., Td we can deﬁne the sum:\\n\\nwhere Θδ is the step function for the open neighborhood:\\n\\nUsing the above sum we can calculate the recurrence frequency along each\\ndiagonal:\\n\\nCdE ,δ,θ =\\n\\nSθ,dE (δ)\\nTdE − θ\\n\\n(67)\\n\\nthe higher this value is, the more the system’s dynamics comes within a δ\\nneighborhood of the periodic orbit with period θ. In the case of (predominan-\\n\\n23\\n\\n−θ(cid:88)\\n\\nTdE\\n\\nu=1\\n\\n(cid:40)\\n\\nSθ,dE (δ) =\\n\\nΘδ (Du+θ,u)\\n\\nΘδ (Du,u(cid:48)) =\\n\\n0, Du,u(cid:48) < δ\\n1, Du,u(cid:48) ≥ δ\\n\\n(65)\\n\\n(66)\\n\\n\\x0ctely) periodic dynamics, as δ decreases, the only diagonals with recurrence\\nhave 100% recurrence (CdE ,δ,θ = 1). This is no longer the case when stochas-\\ntic dynamics emerges at the level of the network’s mean total neural ﬁring\\nenergy, in this case, there may be ﬁnite radii after which there are no lines\\nwith 100% recurrence. In this case, for a given embedding dimension, the\\nresearch on any emergent order present at the level of recurrence patterns\\nmust be analyzed in terms of the diﬀerent recurrence frequencies as the radii\\nare increased.\\n\\nIf the dynamics has a attractor-like structure with a stationary measure,\\nthen, CdE ,δ,θ provides an estimate for the probability of recurrence conditional\\non the periodicity θ. The total recurrence frequency for the points lying in\\nthe diagonals, on the other hand, can be calculated as:\\n\\n2(cid:80)Td\\n\\nCdE ,δ =\\n\\nθ=1 Sθ,dE (δ)\\nT 2\\ndE\\n\\n− TdE\\n\\n(68)\\n\\nwhich corresponds to the proportion of recurrent points under the main di-\\nagonal of the distance matrix. The correlation dimension of a dynamical\\nattractor can be estimated as the slope of the linear regression of log (CdE ,δ)\\non log (δ) for diﬀerent values of δ (Grassberger and Procaccia, 1983a, 1983b;\\nKaplan and Glass, 1995). One can ﬁnd a reference embedding dimension\\nto capture the main structure of an attractor by estimating the correlation\\ndimensions for diﬀerent embedding dimensions and checking for convergence.\\nA third measure that we can use is the probability of ﬁnding a diagonal\\n\\nline with CdE ,δ,θ = 1 given that CdE ,δ,θ > 0:\\n\\nP [CdE ,δ,θ = 1| CdE ,δ,θ > 0] =\\n\\n#{θ : CdE ,δ,θ = 1}\\n#{θ(cid:48) : CdE ,δ,θ(cid:48) > 0}\\n\\n(69)\\n\\nthis corresponds to the probability of ﬁnding a line with 100% recurrence in\\na random selection of lines with recurrence. This measure, provides a picture\\nof stochasticity versus periodic and quasiperiodic recurrences. Indeed, if for\\n\\n24\\n\\n\\x0cthe radius δ there are lines with recurrence and lines with no recurrence,\\nand all the lines with recurrence have CdE ,δ,θ = 1, then, for that radius the\\nrecurrence is either periodic or quasiperiodic, on the other hand the lower the\\nabove probability is the more lines we get without 100% recurrence, which\\nmeans that for that sample data there is a strong presence of divergence from\\nregular periodic or quasiperiodic dynamics. The greater the level of stochastic\\ndynamics the lower the above value is. For emergent chaotic dynamics, given\\na suﬃciently long time (dependent on the largest Lyapunov exponent), all\\ncycles become unstable, which means that the above probability becomes\\nzero, for a suﬃciently long time.\\n\\n3.2 Mean Energy Dynamics of a Thee-Neuron Network\\n\\nLet us consider the QuANN with the following architecture:\\n\\n• N = {N1, N2, N3};\\n• G = {(N2, N1), (N3, N1), (N1, N2), (N1, N3), (N2, N3)};\\n• H⊗3\\n2 ;\\n• L =\\n\\n(cid:110) ˆL1, ˆL2, ˆL3\\n\\n(cid:111)\\n\\n, with ˆL1, ˆL2, ˆL3, respectively, given by:\\n\\nˆL1 = ˆ1 ⊗ (|00(cid:105)(cid:104)00| + |11(cid:105)(cid:104)11|) +\\n+ ˆW ⊗ (|01(cid:105)(cid:104)01| + |10(cid:105)(cid:104)10|)\\n\\nˆL2 = |0(cid:105)(cid:104)0| ⊗ ˆ1 ⊗ ˆ1 + |1(cid:105)(cid:104)1| ⊗ ˆW ⊗ ˆ1\\nˆL3 = (|00(cid:105)(cid:104)00| + |11(cid:105)(cid:104)11|) ⊗ ˆσ1+\\n+ (|01(cid:105)(cid:104)01| + |10(cid:105)(cid:104)10|) ⊗ ˆ1\\n\\n(70)\\n\\n(71)\\n\\n(72)\\n\\nIn this case, there are 6 = 3! alternative neural activations, there is no pair\\nof activation sequences that coincides up a global phase factor.\\n\\n25\\n\\n\\x0cFor the simulations of the neural network, we assume that the environ-\\nment is an ensemble in a maximum (von Neumann) entropy state4 and set\\nthe main initial condition for the environment plus network as:\\n\\n(cid:32)\\n\\n6(cid:88)\\n\\nk=1\\n\\n1\\n6\\n\\n(73)\\n\\n(74)\\n\\n(75)\\n\\n(cid:33)\\n\\nˆρE+N et(0) =\\n\\n|εk(cid:105)(cid:104)εk|\\n\\n⊗ |p(cid:105)(cid:104)p|\\n\\nwhere the density |p(cid:105)(cid:104)p| is deﬁned as:\\n\\n|p(cid:105)(cid:104)p| = ˆU⊗3\\n\\np\\n\\n|000(cid:105)(cid:104)000| ˆU⊗3†\\n\\np\\n\\nwith the operator ˆUp given by:\\n\\nˆUp =(cid:112)1 − pˆσ3 +\\n\\n√\\n\\npˆσ1\\n\\nIf p is set to 1/2 we get the Haddamard transform, so that the initial network’s\\nstate is the pure state |+(cid:105)⊗3, otherwise, we get a biased superposition of ﬁring\\nand nonﬁring for each neuron. In the simulations for the network we assume\\nthe condition expressed in Eq.(4) to hold, since, in this case, the quantum\\nmean for the total neural ﬁring energy coincides numerically (though not in\\nunits) with the quantum mean for the number of ﬁring neurons. Setting the\\nenergy of the neural ﬁring to a diﬀerent value aﬀects the scale of the graphs\\nbut not the resulting dynamics, so there is no loss of generality in the results\\nthat follow.\\n\\nFrom Eqs.(73) to (75), it follows that the greater the value of p is, the\\ngreater is the initial amplitude associated with the neural ﬁring for each\\nneuron, likewise, the lower the value of p is, the lower is this amplitude.\\n\\n4The maximum von-Neumann entropy state for the environment serves two purposes:\\non the one hand, it does not favor a particular direction of activation of the network,\\nallowing us to illustrate how the network behaves with an equally weighted statistical\\nmixture over the diﬀerent activation sequences, on the other hand, it will allow us to show\\nhow, for this type of coupling, the network (as an open system) can make emerge complex\\ndynamics when it processes a target ensemble that is in maximum (von Neumann) entropy.\\n\\n26\\n\\n\\x0cIn ﬁgure 1, we plot the mean total neural ﬁring energy dynamics for\\n\\ndiﬀerent values of p.\\n\\nFigure 1: Mean total neural ﬁring energy dynamics\\n, for diﬀerent\\nvalues of p.\\nIn each case, 10000 iterations are plotted after initial 1000\\niterations, which were dropped out for possible transients. The parameter p\\nproceeds in steps of 0.001, starting at p = 0 up until it reaches 1.\\n\\nl\\n\\n(cid:68) ˆHN et\\n\\n(cid:69)\\n\\nA ﬁrst point that can be noticed is that there are no visible periodic\\nwindows. On the other hand, the network seems to exhibit nonuniform\\nbehavior, namely, there are darker regions in the plot that correspond to\\nconcentrated ﬂuctuations of the network for those values of p and lighter\\nregions that are less “explored”. This implies that the network may tend\\nto show markers of turbulence for diﬀerent values of p associated with an\\nasymmetric behavior. Figure 2 below illustrates this for a value of p near\\n0.9. The ﬂuctuations are concentrated in the region between 1.4J and 1.8J.\\nThen, with less frequency there are those energy ﬂuctuations above 2J, where\\nthe network is more active, the overall dynamics in ﬁgure 2 shows evidence of\\nturbulence in the mean neural activation energy, illustrating ﬁgure 1’s proﬁle\\nfor a speciﬁc value of p.\\n\\n27\\n\\n\\x0cFigure 2: Mean total neural ﬁring energy dynamics\\n, for 1000 iterations\\nof the three-neuron neural network, with p randomly chosen in the interval\\n[0, 1], the value that p obtained for this simulation was 0.8918547337153693.\\n\\nl\\n\\n(cid:69)\\n\\n(cid:68) ˆH3\\n\\nAnother feature evident in ﬁgure 1 is that there is a transition in the\\ndynamics proﬁle. For lower values of p, the distribution for the mean total\\nneural ﬁring energy dynamics is asymmetric negative, that is, the deviations\\ncorrespond to lower energy peaks. As p approaches a region between 0.2\\nand 0.5, there is a bottleneck, where the dynamics becomes more uniformly\\ndistributed showing less dispersion of values. When p rises further, the sym-\\nmetry changes with the peaks corresponding to higher activation energy.\\n\\nWhile the standard time series plot for\\n\\nallows us to picture the\\ntemporal evolution of the mean total energy. A delay embedding in three-\\ndimensional space allows us to visualize, geometrically, possible emergent\\npatterns for the mean energy, providing a geometric picture of the result-\\ning emergent dynamics.\\nIn ﬁgure 3, we show the result of an embedding\\nof the neural network’s mean total neural ﬁring energy dynamics in three\\ndimensional Euclidean space, for the same value of p as in ﬁgure 2.\\n\\nl\\n\\n(cid:69)\\n\\n(cid:68) ˆH3\\n\\n28\\n\\n\\x0cFigure 3: Delay coordinate embedding of the mean total neural ﬁring energy\\ndynamics for p = 0.8918547337153693. For the time delay embedding we\\nused a lag of 1 since this is the ﬁrst zero crossing of the autocorrelation\\nfunction, the embedding was obtained from 105 iterations after 1000 initial\\niterations discarded for transients.\\n\\nThe embedded dynamics shows evidence of a complex structure. To ad-\\ndress this structure we estimated ﬁrst the correlation dimensions for diﬀerent\\nembedding dimensions. Table 1 in appendix shows the correlation dimensions\\nestimated for four sequential epochs, each epoch containing 1000 embedded\\npoints. The estimates’ proﬁles are the same in the four epochs: for each em-\\nbedding dimension, we get a statistically signiﬁcant estimation, with an R2\\naround 99% and there is a convergence to a correlation dimension between 4\\nand 5 dimensions, with a slowing down of the diﬀerences between each esti-\\nmated correlation dimension, as the embedding dimension approximates the\\nrange from dE = 6 to dE = 9. In this range, dE = 7 has the lowest standard\\nerror.\\n\\nConsidering a delay embedding with dE = 7, table 2, in appendix, shows\\nthe estimated recurrence frequencies (expressed in percentage) calculated\\n\\n29\\n\\n\\x0cfor each diagonal line of the distance matrix, for increasing radii, with the\\nradii deﬁned proportionally to the non-embedded sample series’ standard-\\ndeviation (in this case, a 5000 data points’ series).\\n\\nThe recurrence structure reveals that the mean energy dynamics has el-\\nements of dynamical stochasticity.\\nIndeed, for radii between 0.5 and 0.7\\nstandard-deviations the maximum percentage of diagonal line recurrence\\nranges from around 39% to around 89%, this means that the embedded\\ntrajectory is not periodic but there is at least one cycle with high recurrence\\n(around 39%, in the case of 0.5 standard-deviations, around 68%, in the case\\nof 0.6 standard-deviations, and around 89%, in the case of 0.7 standard-\\ndeviations). The mean cycle recurrence is, however, for this range of radii,\\nvery small, less that 1%, the median is 0% which means that half the diagonal\\nlines have 0% recurrence and the other half have more than 0% recurrence,\\nthe standard-deviation of the recurrence percentage is also small.\\n\\nSince, for a low radius, we do not have a full line with 100% recurrence,\\nthe dynamics, for the embedded sample trajectory, is not periodic. This\\nproﬁle changes as the radius is increased, indeed, as the radius is increased, a\\nfew number of diagonal lines with 100% recurrence start to appear, following\\na power law progression5. For a radius of 2 standard-deviations we get 26\\nlines with 100% recurrence, we also get a median recurrence percentage of\\n4.1322% and a mean recurrence percentage of 8.8508%, wich means that the\\npercentage of recurrence along the diﬀerent cycles tends to be low.\\n\\nThe lines with 100% recurrence are not evenly separated, pointing to-\\nwards an emergent quasiperiodic structure. The fact that a quasiperiodic\\nrecurrence pattern only appears for a rising radius, and the low (on average)\\nrecurrence indicates that the system has an emergent stochastic dynami-\\ncal component and, simultaneously, persistent recurrence signatures that are\\nproper of quasiperiodic dynamics, intermixing dynamical stochasticity and\\n\\n5The number of diagonal lines with 100% recurrence N100%, scales, in this case, as:\\nN100% = 0.847567181δ3.480611609 (R2 = 0.960241606, p − value = 4.73894e − 09, S.E. =\\n0.213542037).\\n\\n30\\n\\n\\x0cpersistent quasiperiodic recurrences.\\n\\nThis is illustrated in ﬁgure 4, where we show a recurrence plot for a radius\\nof 2 standard-deviations dE = 7 and a simulation comprised of 10000 data\\npoints. In the recurrence plot, we see the predominance of almost isolated\\ndots typical of noise data, broken diagonal lines indicating unstable cycles\\ntypical of unstable divergence that takes place in chaotic dynamics, and the\\nlong full diagonal lines with 100% recurrence and unneven spacing, which is\\ntypical of quasiperiodic recurrences.\\n\\nFigure 4: Recurrence plot for the table 2 data, with the radius of 2 standard-\\ndeviations, the embedded trajectory pairs with (cid:107)xu − xu(cid:48)(cid:107) < δ are marked\\nin black, the other points are marked in white. The diagram’s orientation is\\nfrom top to bottom and left to right, following the structure of the distance\\nmatrix.\\n\\nIn computer science and complex systems science, the dynamical regime\\nthat most closely matches the above dynamics is captured by the concept\\nof edge of chaos, where stochastic dynamics and persistent regular dynamics\\ncoexist, expanding the systems’ adaptive ability due to a self-organization in\\nan intermediate region between regular and stochastic dynamics (Kauﬀman\\nand Johnsen, 1991; Kauﬀman, 1993). The intermixing of stochasticity and\\n\\n31\\n\\n\\x0cregular dynamics, as addressed above, show up as the radius is increased,\\nindeed, for a low radius, the dynamics only shows noisy recurrences, as the\\nradius is increased, however, resilient quasiperiodic recurrences, in the form\\nof unevenly spaced diagonal lines with 100% recurrence, start to appear\\nin the recurrence structure of the embedded series, following a power-law\\nprogression.\\n\\nThe quasiperiodic recurrences constitute, in this case, a form of noise\\nresilient dynamical record, which may have possible applications in quantum\\ntechnologies, namely in regards to the ability for a networked open quantum\\nsystem to dynamically encode patterns in resilient recurrences. Table 3, in\\nappendix, illustrates the level of resilience associated to the quasiperiodic\\nrecurrences. For that table, we divided 100000 iterations of the network\\ninto ﬁve epochs of 10000 iterations each and calculated the diagonal line\\nrecurrences on the delay-embedded series using a ﬁxed radius of δ = 0.4\\n(slightly below 1.7 standard-deviations) and an embedding dimension of 7.\\nOn average, the diagonal line recurrence frequencies are low for each\\nepoch (around 4.3%) with a standard-deviation around 9.8%, which conﬁrms\\nthe presence of dynamical stochasticity with a high dispersion around the\\nmean. We identiﬁed, however, a ﬁxed number of 28 diagonal lines with 100%\\nrecurrence that remain the same for each epoch, and represent 0.5607% of\\nthe total number of diagonal lines. The lowest period with 100% recurrence\\nis 157, and the highest is 9871.\\n\\nAs shown in table 3, the observed distances between cycles ranges from\\n2 to 1111 embedded points, which shows the unevenness of the distances\\nbetween the 100% recurrence lines, typical of quasiperiodic dynamics. The\\nmean distance between the lines with 100% recurrence is 359.778.\\n\\nTable 4, in appendix, shows the distribution of distances between the\\ndiagonal lines. There are two dominant distances which represent roughly\\n51.85% of the distribution: 157 (which occurs 9 times) and 389 (which oc-\\ncurs 5 times), besides these two main dominant distances, we get a uniform\\n\\n32\\n\\n\\x0cdistribution on the distances 26, 131, 363, 562, 722 and 1111, and an adi-\\ncional single case of a distance equal to 520. The fact that there is no third\\ndominant distance, as occurs for a standard quasiperiodic motion on a torus\\n(Zou, 2007), and the uniform distribution over an unneven set of distances\\nrepresenting about 44.44% of the distance distribution indicates a form of\\nemergent erratic quasiperiodicity 6, present at that radius.\\n\\nWe can trace back the interplay between dynamical stochasticity and\\nemergent persistent quasiperiodicity, present in the mean total neural ﬁring\\nenergy dynamics, to the network’s cognition of the environment. Indeed, in\\nthis case, since the environment is an ensemble in a maximum (von Neumann)\\nentropy distribution over the eigenstates |εk(cid:105)(cid:104)εk|, the ﬁnal dynamics results\\nfrom the mixture over the diﬀerent neural activation orders.\\n\\nIf we consider the processing of each environment’s eigenstate by the net-\\nwork, then, we can identify the diﬀerences in the network’s dynamics which\\ncontribute to the ﬁnal emergent pattern identiﬁed above. For instance, for\\nthe environment’s state |ε1(cid:105)(cid:104)ε1|, we obtain a more regular attractor structure\\n(ﬁgure 5, left) than for the environment’s state |ε5(cid:105)(cid:104)ε5| (ﬁgure 5, right).\\n\\n6A concept of erratic quasiperiodicity associated to irregular quasiperiodic sequences\\nof recurrences was discussed by Haake et al. (1987) in the context of quantum chaos for\\na kicked top.\\n\\n33\\n\\n\\x0cFigure 5: Delay coordinate embedding of the mean total neural ﬁring energy\\ndynamics for p = 0.8918547337153693, with the environment’s initial state\\nset to the basis state |ε1(cid:105)(cid:104)ε1| (left) and in the basis state |ε5(cid:105)(cid:104)ε5| (right).\\nFor the time delay embedding, we used a lag of 1 since this is the ﬁrst zero\\ncrossing of the autocorrelation function, the embedding was obtained from\\n100000 iterations after 1000 initial iterations discarded for transients.\\n\\nTable 5, in appendix, illustrates the recurrences obtained for each en-\\nvironment’s eigenstate using, for comparison purposes, a seven dimensional\\ndelay embeding of 10000 iterations of the network, and deﬁning a radius\\nδ = 0.4. From table 5, it follows that the dynamics depends critically on\\nthe feedforward and backpropagation neural processing triggered by the en-\\nvironment. Indeed, there are more regular dynamics triggered, respectively,\\nby the network’s processing of the environment in the states |ε1(cid:105)(cid:104)ε1| and\\n|ε6(cid:105)(cid:104)ε6|, leading, in both cases, to a total number of 105 diagonal lines with\\n100% recurrence, and a mean recurrence around 2.5%.\\n\\nIn the network’s processing of the environment in the state |ε1(cid:105)(cid:104)ε1|, there\\nis ﬁrst a feedforward activation, where N1’s state is transformed condition-\\nally on N2 and N3’s states, then, there is a sequence of two backpropagation\\ntransformations from N1 to N2 (completing the recurrent loop) and to N3\\n(completing the second recurrent loop), where N3’s state is transformed con-\\n\\n34\\n\\n\\x0cditionally on N1 (recurrent activation) and on N2 (which, in this case, is a\\nfeedforward activation7).\\n\\nIn the case of the network’s processing of the environment state |ε6(cid:105)(cid:104)ε6|,\\nthere are two feedforward transformations: the transformation of N3’s state,\\nconditional on the other two neurons, then, there is a second feedforward\\nactivation where N2’s state is transformed conditionally on N1. After these\\ntwo feedforward activations there is a backpropagation from N2 and N3 to the\\nneuron N1 closing the recurrent loops. The element in common to the neural\\nprocessing of the states |ε1(cid:105)(cid:104)ε1| and |ε6(cid:105)(cid:104)ε6| is the second transformation\\nwhich is, in both cases, N1 → N2.\\n\\nA similar pattern is present in the other dynamical proﬁles. Thus, the\\nneural processing of the states |ε2(cid:105)(cid:104)ε2| and |ε4(cid:105)(cid:104)ε4| lead to the lowest re-\\ncurrence, for those embedding parameters, with 6 and 7 diagonal lines with\\n100% recurrence, and a mean recurrence around 0.57%, furthermore, the\\ndiagonal lines with 100% recurrence are less resilient with divergence occur-\\nring for repeated simulations in sequential epochs of size 10000. These two\\nlow recurrence dynamics are also characterized by the same second neural\\nactivation.\\n\\nIndeed, the neural processing of the environment in the state |ε2(cid:105)(cid:104)ε2| is\\nsuch that the feedforward transformation N2 → N1 ← N3 is activated ﬁrst,\\nthen, follows the activation of the neural circuit N1 → N3 ← N2, where N1\\nis feeding backward (backpropagation) to N3 and N2 is feeding forward to\\nN3, ﬁnally there is a feeding backward from N1 to N2.\\n\\nSimilarly, the neural processing of the environment in the state |ε4(cid:105)(cid:104)ε4|,\\nis such that N1 ﬁrst feeds forward to N2, and then both neurons feed forward\\nto N3 (following the circuit N1 → N3 ← N2), and then N2 and N3 feed\\nbackward to N1.\\n\\n7In this sense, the last transformation has both a backpropagation and feedforward\\n\\ndynamics.\\n\\n35\\n\\n\\x0cThe third pair of dynamics comes from the neural processing of the envi-\\nronment in the states |ε3(cid:105)(cid:104)ε3| and |ε5(cid:105)(cid:104)ε5|. Again, the second transformation\\ncoincides. Indeed, when the environment is in the state |ε3(cid:105)(cid:104)ε3|, the neural\\nnetwork ﬁrst activates the feedforward connection N1 → N2, then the neural\\ncircuit N2 → N1 ← N3 is activated, where N2 is feeding backward to N1 and\\nN3 is feeding forward to N1, after this transformation the ﬁnal neural circuit\\nN1 → N3 ← N2 is activated, where N1 feeds backward to N3 and N2 feeds\\nforward to N3. When the environment is in the state |ε5(cid:105)(cid:104)ε5|, the feedforward\\ndirection is N1 → N3 ← N2, followed by the neural circuit N2 → N1 ← N3\\nactivation, where N3 is feeding backward to N1 and N2 is feeding forward,\\nﬁnally there is a last transformation where N1 feeds backward to N2.\\n\\nIn both of these cases we, again, have the same second transformation,\\nthe number of lines with 100% recurrence obtained are equal to 10 and the\\nmean recurrence is around 1%.\\n\\nThis shows how the relation between the way in which information ﬂows\\nin the network in its processing of the environment can have an eﬀect on the\\nnetwork’s dynamics. Besides the initial state of the environment, the initial\\nstate of the network also matters. The above examples were worked from p\\nin the region of values above the bottleneck shown in ﬁgure 1.\\n\\nIf the value of p is lowered, then, the quasiperiodic order becomes more\\nsigniﬁcant even though there is always a strong presence of stochastic dynam-\\nics, so that for lower values of p we get emergent torus-shaped attractors, as\\nis shown in ﬁgure 6’s example.\\n\\n36\\n\\n\\x0cFigure 6: Delay coordinate embedding of the mean total neural ﬁring energy\\ndynamics, with p = 0.19232544805500018. For the time delay embedding we\\nused a lag of 1 since this is the ﬁrst zero crossing of the autocorrelation func-\\ntion, the embedding was obtained from 100000 iterations after 1000 initial\\niterations discarded for transients, the initial state is described by Eq.(73).\\n\\nIn ﬁgure 7, we plot the conditional probabilities of getting a 100% re-\\ncurrence line from a random selection of lines with recurrence points (these\\ncorrespond to the probabilities P [ CdE ,δ,θ = 1| CdE ,δ,θ > 0] deﬁned in Eq.(69)),\\nusing a radius of 1 standard-deviation, for diﬀerent values of p and diﬀerent\\nembedding dimensions and for cycle lengths from an original series of 2000\\ndata points (thus, we get the probability of 100% recurrence, given that the\\nline shows recurrence for low period cycles).\\n\\n37\\n\\n\\x0c(cid:69)\\n\\n(cid:68) ˆH3\\n\\nl\\n\\nFigure 7: Probability of 100% recurrence diagonal line given that the line\\ncontains recurrence points, for the mean total neural ﬁring energy dynamics\\n, applying a delay embedding on a 2000 iterations’ simulation of the\\nnetwork, after initial 1000 iterations dropped for transients. The parameter\\np proceeds in steps of 0.001, starting at p = 0 up until it reaches 1, the\\nembedding dimensions range from 3 to 8 and the delay period is in each case\\nset as the ﬁrst zero crossing of the autocorrelation function. The radius δ\\nwas set to 1 standard-deviation\\n\\nFor all cases, shown in ﬁgure 7, the probability is less than 1, which means\\nthat the system always exhibits a combination of stochastic dynamics and\\nquasiperiodic recurrences, however, the probability of ﬁnding 100% recur-\\nrence lines in a random selection of lines with recurrent points, for an open\\nneighborhood size of 1 standard-deviation, increases for a region of p that\\ngoes from below 0.1 up to 0.2 and then again, although with lower values,\\naround 0.3 and 0.4 (which includes the bottleneck region).\\nIn these cases\\nwe get emergent toroidal attractors with evidence of noisy quasiperiodic dy-\\nnamics. These regions are the ones in which the network produces a more\\nresilient quasiperiodic dynamics.\\n\\nAs p is raised, the quasiperiodicity tends to be less representative and the\\nnetwork tends progressively to a form of emergent stochastic dynamics. Even\\nthough, for high value of p, as we showed above, we still get edge of chaos-like\\n\\n38\\n\\n\\x0cdynamical markers, the level of proximity to a stochastic dynamics increases\\nwith the initial amplitude associated with each neuron’s active ﬁring state,\\nthe higher this amplitude is, the more the network tends to a predominantely\\nstochastic dynamics.\\n\\n4 Conclusion\\n\\nAs models of networked quantum computation, QuANNs introduce novel fea-\\ntures for both quantum computer science as well as for machine learning in\\ngeneral. While a QuANN can be designed such that a certain computational\\nproblem is solved by parallel networked quantum processors, the main pur-\\npose of a QuANN, as a computational system, is to go beyond a programmed\\nnetworked quantum computer.\\n\\nNamely, a QuANN should work as an artiﬁcial cognitive system that is\\ncapable of evaluating the task with respect to a certain computational goal\\nand select accordingly the best quantum circuits to implement that allow it\\nto solve the problem eﬃciently.\\n\\nThe ﬁrst part of the present work was aimed at the implementation of\\nthis type of quantum neural cognition, using a feedforwad neural network\\nstructure with quantum backpropagation. In this case, the output neurons\\ntake on the task of evaluating the input ﬁring patterns with respect to the\\nintended task, this is the ﬁrst stage of the neural processing dynamics, where\\nthe network’s computation tends, during the neural learning temporal inter-\\nval, towards a quantum circuit that is adapted to the computational problem.\\nThe second stage of the task (the backpropagation stage) is to conditionally\\ntransform the input layer’s state so as to solve the problem.\\n\\nThe quantum backpropagation eﬀectively implements a form of adaptive\\nerror correction, in the sense that the input layer’s state is conditionally\\ntransformed in such a way that it exhibits the ﬁring patterns that solve the\\ncomputational problem. This is illustrated in both examples provided in sec-\\n\\n39\\n\\n\\x0ction 2, where, for the ﬁrst problem (subsection 2.1), the network adaptively\\ntransforms the input layer so that it exhibits a speciﬁc ﬁring pattern, and,\\nfor the second problem, the network adaptively transforms the input layer\\nso that it represents a general n-to-m Boolean function, exemplifying the\\napplication of the neural learning scheme to a computer science problem.\\nThe ﬁrst problem shows how the quantum network can adaptively set the\\nenergy levels of each neuron in a layer, an ability that may be relevant for\\nfuture quantum networked technologies, namely, in what regards the ability\\nof quantum networks to self-manage energy levels.\\n\\nWhile the framework, addressed in section 2, introduces a quantum neu-\\nral learning scheme to adaptively solve speciﬁc computational problems, in\\nsection 3, the feedforward and backpropagation take on a role as building\\nblocks for an unsupervised dynamical neural processing of an environment,\\nwith the conditional neural state transition taking place with respect to the\\ndiﬀerent neural activation orders, thus extending the formalism of quantum\\ncomputation with changing orders of gates (Procopio, et al., 2015; Brukner,\\n2014; Chiribella, et al., 2013; Aharonov, et al., 1990) to the context of quan-\\ntum machine learning.\\n\\nThe example shown in section 3, recovers, in a quantum setting, speciﬁc\\nfeatures that have been addressed in classical Hopﬁeld networks and classical\\nmodels of neural computation, namely the relation with chaos as a way to\\nextend the memory capacity of neural networks in learning new patterns\\n(Watanabe et al., 1997; Akhmet and Fen, 2014) as well as the ability of ANNs\\nto simulate neural synchronization and adaptive dynamics taking advantage\\nof a transition between chaos and quasiperiodic recurrences (Akhmet and\\nFen, 2014).\\n\\nIn the network simulated in section 3, we get a simultaneous presence\\nof two levels of emergent dynamics: a stochastic dynamics and a resilient\\nquasiperiodic dynamics, evident in the recurrence statistics. The higher\\nthe initial amplitude for each neuron to be in the active state, the stronger\\n\\n40\\n\\n\\x0cthe stochastic dynamics is, with the recurrence patterns showing markers of\\nstochastic noise-like dynamics, however, with a suﬃciently high radius used\\nfor the neighborhood structure analysis, resilient quasiperiodic recurrences\\nstart to appear, so that the network’s mean neural ﬁring energy dynam-\\nics exhibits a coexistence of stochastic dynamics and resilient quasiperiodic\\ndynamics.\\n\\nThe quasiperiodic dynamics becomes more dominant when the initial\\namplitude for each neuron to be in the nonﬁring state is high, for these values\\ntoroidal attractors appear, although there is still evidence of the presence of\\nstochastic dynamics.\\n\\nThe network exhibits, thus, a self-organization towards a dynamical regime\\nwhere it is able to sustain a persistent order alongside stochastic dynamics,\\nintermixing both randomness and resilient quasiperiodic dynamical patterns\\nthat constitute a form of noise resilient dynamical record. The dynami-\\ncal markers are typical of the concept of edge of chaos. At the edge of\\nchaos, a complex adaptive system is simultaneously capable of change and\\nstructure conservation, producing the order it needs to survive, maximiz-\\ning its adaptability since it neither falls into a rigid unchanging structure\\n(ceasing to adapt) nor falls into the opposite chaotic side leading to a col-\\nlapse of conserved structures (Packard, 1988; Langton, 1990; Crutchﬁeld and\\nYoung, 1990; Kauﬀman and Johnsen, 1991; Kauﬀman, 1993). Future work\\non QuANN dynamics is needed in order to produce a theory of open system’s\\nquantum cognition in the context of quantum machine learning research, in\\nparticular in what regards forms of distributed system-wide awareness in\\nlarge QuANNs and dynamical memory formation.\\n\\nAharonov Y, Anandan J, Popescu S and Vaidman L. Superpositions of\\ntime evolutions of a quantum system and a quantum time-translation ma-\\nchine. Phys. Rev. Lett. 1990; 64(25):2965-2968.\\n\\nAkhmet M and Fen MO. Generation of cyclic/toroidal chaos by Hopﬁeld\\n\\nneural networks. Neurocomputing, 2014; 145:230-239.\\n\\n41\\n\\n\\x0cBehrman EC, Niemel J, Steck JE, Skinner SR. A Quantum Dot Neural\\nNetwork, In: T. Toﬀoli T and M. Biafore, Proceedings of the 4th Workshop\\non Physics of Computation, Elsevier Science Publishers, Amsterdam, 1996;\\n22-24.\\n\\nBertschinger N and Natschläger T. Real-time computation at the edge of\\n\\nchaos in recurrent neural networks. Neural Comput., 2004; 16(7):1413-36.\\n\\nBrukner Č. Quantum causality. Nature Physics, 2014; 10:259-263.\\nChiribella G, D’Ariano GM, Perinotti P and Valiron B. Quantum compu-\\ntations without deﬁnite causal structure. Phys. Rev. A, 2013; 88(2):022318.\\nChrisley R. Quantum learning, In: Pylkkänen P and Pylkkö P (eds.),\\nNew directions in cognitive science: Proceedings of the international sympo-\\nsium, Saariselka, 4-9 August, Lapland, Finland, Finnish Artiﬁcial Intelligence\\nSociety, Helsinki, 1995; 77-89.\\n\\nCrutchﬁeld JP and Young K. Computation at the Onset of Chaos, In:\\nZurek W. Entropy, Complexity, and the Physics of Information. SFI Studies\\nin the Sciences of Complexity. Proceedings Volume VIII in the Sante Fe\\nInstitute Studies in the Sciences of Complexity, Addison-Wesley, Reading,\\nMassachusetts, 1990; 223-269.\\n\\nGonçalves CP. Quantum Cybernetics and Complex Quantum Systems\\nScience - A Quantum Connectionist Exploration. NeuroQuantology, 2015a;\\n13(1): 35-48.\\n\\nGonçalves CP. Financial Market Modeling with Quantum Neural Net-\\n\\nworks. Review of Business and Economics Studies, 2015b; 3(4):44-63.\\n\\nGorodkin J, Sørensen A and Winther O. Neural Networks and Cellular\\n\\nAutomata Complexity. Complex Systems, 1993, 7:1-23.\\n\\nGrassberger P and Proccacia I. Characterization of strange attractors.\\n\\nPhys. Rev. Lett, 1983a; 50:346-349.\\n\\nGrassberger P and Proccacia I. Measuring the Strangeness of Strange\\n\\nAttractors. Physica D, 1983b; 9:189-208.\\n\\nHaake F, Kuś M and Scharf R. Classical and Quantum Chaos for a Kicked\\n\\n42\\n\\n\\x0cTop. Z. Phys. B. - Condensed Matter, 1987; 65:381-395.\\n\\nIvancevic VG and Ivancevic TT. Quantum Neural Computation, Springer,\\n\\nDordrecht, 2010.\\n\\nKak S. Quantum Neural Computing. Advances in Imaging and Electron\\n\\nPhysics, 1995; 94:259-313.\\n\\nKaplan D and Glass L. Understanding Nonlinear Dynamics. Springer,\\n\\nNew York, 1995.\\n\\nKauﬀman SA and Johnsen S. Coevolution to the Edge of Chaos: Coupled\\nFitness Landscapes, Poised States, and Coevolutionary Avalanches. J. theor.\\nBiol., 1991; 149:467-505.\\n\\nKauﬀman SA. The Origins of Order: Self-Organization and Selection in\\n\\nEvolution. Oxford University Press, New York, 1993.\\n\\nLangton C. Computation at the Edge of Chaos: Phase Transitions and\\n\\nEmergent Computation. Physica D, 1990; 42:12-37.\\n\\nMenneer T and Narayanan A. Quantum-inspired Neural Networks, tech-\\nnical report R329, Department of Computer Science, University of Exeter,\\nExeter, United Kingdom, 1995.\\n\\nMenneer T Quantum Artiﬁcial Neural Networks, Ph. D. thesis, The\\n\\nUniversity of Exeter, UK, 1998.\\n\\nNayfeh, AH and Balachandran, B. Applied Nonlinear Dynamics - Ana-\\nlytical, Computational and Experimental Methods, Wiley-VCH, Germany,\\n2004.\\n\\nPackard, NH. Adaptation toward the edge of chaos. University of Illinois\\n\\nat Urbana-Champaign, Center for Complex Systems Research, 1988.\\n\\nProcopio LM, Moqanaki A, Araújo M, Costa F, Calafell IA, Dowd EG,\\nHamel DR, Rozema LA, Brukner Č and Walther P. Experimental superpo-\\nsition of orders of quantum gates. Nature Communications, 2015; 6:7913.\\n\\nSchuld M, Sinayskiy I and Petruccione F. The quest for a Quantum Neural\\n\\nNetwork. Quantum Information Processing, 2014a; 13(11): 2567-2586.\\n\\nSchuld M, Sinayskiy I and Petruccione F. Quantum walks on graphs rep-\\n\\n43\\n\\n\\x0cresenting the ﬁring patterns of a quantum neural network. Phys. Rev. A,\\n2014b; 89: 032333.\\n\\nWatanabe M, Aihara K and Kondo S. Self-organization dynamics in\\n\\nchaotic neural networks. Control Chaos Math. Model., 1997; 8:320-333\\n\\nWolfram S. A New Kind of Science. Wolfram Research, Canada, 2002.\\nZou Y. Exploring Recurrences in Quasiperiodic Dynamical Systems. Ph.\\n\\nD. Thesis, University of Potsdam, 2007.\\n\\n44\\n\\n\\x0cAppendices - Tables\\n\\ndE\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\nEpoch 1\\n\\nD2 (cid:39) 2.1051\\nR2 (cid:39) 0.9992\\nsig. (cid:39) 2.35e − 07\\nS.E. (cid:39) 0.0296\\nD2 (cid:39) 2.8127\\nR2 (cid:39) 0.9991\\nsig. (cid:39) 2.75e − 07\\nS.E. (cid:39) 0.0412\\nD2 (cid:39) 3.5127\\nR2 (cid:39) 0.9992\\nsig. (cid:39) 2.15e − 07\\nS.E. (cid:39) 0.0483\\nD2 (cid:39) 4.0670\\nR2 (cid:39) 0.9996\\nsig. (cid:39) 5.69e − 08\\nS.E. (cid:39) 0.0401\\nD2 (cid:39) 4.4239\\nR2 (cid:39) 0.9999\\nsig. (cid:39) 1.23e − 09\\nS.E. (cid:39) 0.0168\\nD2 (cid:39) 4.5664\\nR2 (cid:39) 0.9999\\nsig. (cid:39) 5.07e − 09\\nS.E. (cid:39) 0.0246\\nD2 (cid:39) 4.300\\nR2 (cid:39) 0.9986\\nsig. (cid:39) 7.84e − 06\\nS.E. (cid:39) 0.0818\\n\\nEpoch 2\\n\\nD2 (cid:39) 2.1295\\nR2 (cid:39) 0.9992\\nsig. (cid:39) 2.35e − 07\\nS.E. (cid:39) 0.0300\\nD2 (cid:39) 2.8327\\nR2 (cid:39) 0.9993\\nsig. (cid:39) 1.61e − 07\\nS.E. (cid:39) 0.0363\\nD2 (cid:39) 3.5111\\nR2 (cid:39) 0.9995\\nsig. (cid:39) 8.36e − 08\\nS.E. (cid:39) 0.0382\\nD2 (cid:39) 4.0916\\nR2 (cid:39) 0.9998\\nsig. (cid:39) 1.51e − 08\\nS.E. (cid:39) 0.0290\\nD2 (cid:39) 4.4417\\nR2 (cid:39) 0.9999\\nsig. (cid:39) 1.73e − 09\\nS.E. (cid:39) 0.0182\\nD2 (cid:39) 4.4989\\nR2 (cid:39) 0.9999\\nsig. (cid:39) 4.06e − 07\\nS.E. (cid:39) 0.0726\\nD2 (cid:39) 4.2369\\nR2 (cid:39) 0.9964\\nsig. (cid:39) 4.92e − 06\\nS.E. (cid:39) 0.1277\\n\\nEpoch 3\\n\\nD2 (cid:39) 2.1001\\nR2 (cid:39) 0.9991\\nsig. (cid:39) 3.24e − 07\\nS.E. (cid:39) 0.0321\\nD2 (cid:39) 2.7935\\nR2 (cid:39) 0.9990\\nsig. (cid:39) 3.89e − 07\\nS.E. (cid:39) 0.0363\\nD2 (cid:39) 3.4820\\nR2 (cid:39) 0.9991\\nsig. (cid:39) 2.74e − 07\\nS.E. (cid:39) 0.0509\\nD2 (cid:39) 4.0690\\nR2 (cid:39) 0.9997\\nsig. (cid:39) 3.43e − 08\\nS.E. (cid:39) 0.0354\\nD2 (cid:39) 4.4001\\nR2 (cid:39) 0.9997\\nsig. (cid:39) 2.70e − 08\\nS.E. (cid:39) 0.0360\\nD2 (cid:39) 4.4183\\nR2 (cid:39) 0.9981\\nsig. (cid:39) 1.32e − 06\\nS.E. (cid:39) 0.0958\\nD2 (cid:39) 4.0665\\nR2 (cid:39) 0.9953\\nsig. (cid:39) 8.15e − 06\\nS.E. (cid:39) 0.1391\\n\\nEpoch 4\\n\\nD2 (cid:39) 2.1230\\nR2 (cid:39) 0.9990\\nsig. (cid:39) 3.92e − 07\\nS.E. (cid:39) 0.0340\\nD2 (cid:39) 2.8550\\nR2 (cid:39) 0.9989\\nsig. (cid:39) 4.69e − 07\\nS.E. (cid:39) 0.0478\\nD2 (cid:39) 3.5544\\nR2 (cid:39) 0.9992\\nsig. (cid:39) 2.55e − 07\\nS.E. (cid:39) 0.0511\\nD2 (cid:39) 4.1488\\nR2 (cid:39) 0.9998\\nsig. (cid:39) 1.40e − 08\\nS.E. (cid:39) 0.0354\\nD2 (cid:39) 4.4438\\nR2 (cid:39) 0.9998\\nsig. (cid:39) 1.13e − 08\\nS.E. (cid:39) 0.0293\\nD2 (cid:39) 4.4756\\nR2 (cid:39) 0.9983\\nsig. (cid:39) 1.04e − 06\\nS.E. (cid:39) 0.0914\\nD2 (cid:39) 4.2330\\nR2 (cid:39) 0.9961\\nsig. (cid:39) 5.71e − 06\\nS.E. (cid:39) 0.1324\\n\\nTable 1: Correlation dimensions estimated for four epochs of 1000 embedded\\npoints each, the epochs were obtained after 1000 iterations initially dropped\\nfor transients, with p = 0.8918547337153693, and a lag of 1. In each case, the\\nradius ranges in the region of power-law scaling, between 1 sample standard-\\ndeviation up to 1.7 sample standard-deviations, with steps of 0.1 standard-\\ndeviations.\\n\\n45\\n\\n\\x0cδ/σ\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\n1.1\\n1.2\\n1.3\\n1.4\\n1.5\\n1.6\\n1.7\\n1.8\\n1.9\\n2\\n\\nMax\\n\\nMin\\n\\nMedian\\n\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n\\nStd. Dev. #Lines (100% Rec.)\\nMean\\n0.6161%\\n39.6761% 0% 0.0217%\\n1.2144%\\n68.6640% 0% 0.0557%\\n1.8920%\\n89.9595% 0% 0.1099%\\n2.7087%\\n0% 0.1951%\\n3.6264%\\n0% 0.3224%\\n4.2603%\\n0% 0.4885%\\n0% 0.7173% 0.0497% 4.8634%\\n0% 1.0312% 0.1285% 5.5636%\\n0% 1.4541% 0.2553% 6.3387%\\n0% 2.0026% 0.4564% 7.1779%\\n0% 2.6985% 0.7475% 8.0889%\\n0% 3.5636% 1.1313% 9.0840%\\n0% 4.5100% 1.6380% 10.1223%\\n0% 5.8131% 2.2629% 11.1719%\\n0% 7.2334% 3.0865% 12.2404%\\n0% 8.8508% 4.1322% 13.3660%\\n\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n\\n0\\n0\\n0\\n1\\n1\\n3\\n5\\n5\\n6\\n7\\n9\\n12\\n14\\n17\\n21\\n26\\n\\nTable 2: Recurrence frequencies calculated for the three neuron neural net-\\nwork, the frequencies were calculated for a delay embedding with dE = 7 for a\\n5000 iterations series taken from a 6000 iterations simulation, with the ﬁrst\\n1000 iterations removed for transients and p = 0.8918547337153693. The\\nradii presented were obtained from the standard-deviation of the original se-\\nries ranging from 0.5 standard deviations to up to 2 standard-deviations in\\nsteps of 0.1 standard-deviations.\\n\\n46\\n\\n\\x0cEpoch Mean\\n4.2885\\n4.2868\\n4.2832\\n4.3016\\n4.3052\\n4.2830\\n4.3096\\n4.2967\\n4.2680\\n4.3235\\n\\nE1\\n9.8745\\nE2\\n9.8370\\nE3\\n9.8236\\nE4\\n9.8620\\nE5\\n9.8470\\nE6\\n9.8607\\nE7\\n9.8459\\nE8\\n9.8299\\nE9\\n9.8475\\nE10\\n9.8846\\nLines with 100% Recurrence Statistics\\n\\nStd. Dev.\\n\\nMedian\\n1.5046\\n1.4957\\n1.5021\\n1.5035\\n1.5094\\n1.5038\\n1.5006\\n1.5062\\n1.4858\\n1.5038\\n\\n# Lines with 100% recurrence\\n% Lines with 100% recurrence\\n\\n28\\n\\n0.5607%\\n\\nMin Period\\nMax Period\\nMin Distance\\nMax Distance\\nMean Distance\\n\\n157\\n9871\\n\\n2\\n\\n1111\\n\\n359.778\\n\\nTable 3: Diagonal line recurrence statistics for 10 sequential epochs’ division\\nof 100000 iterations of the neural network, with 10000 mean total ﬁring\\nenergy data points for each epoch size. The statistics were calculated for\\na delay embedding of each epoch’s data using dE = 7 and δ = 0.4, and\\np = 0.8918547337153693.\\n\\n47\\n\\n\\x0cDistance Frequency\\n\\n26\\n131\\n157\\n363\\n389\\n520\\n565\\n722\\n1111\\nTotal\\n\\n2\\n2\\n9\\n2\\n5\\n1\\n2\\n2\\n2\\n27\\n\\nTable 4: Distance distribution for the 100% recurrence lines identiﬁed in the\\nprevious table’s simulations.\\n\\nEnvironment Permutations # Lines Mean Rec.\\n2.4952%\\n0.5670%\\n1.0225%\\n0.5703%\\n1.0249%\\n2.4996%\\n\\n|ε1(cid:105)(cid:104)ε1|\\n|ε2(cid:105)(cid:104)ε2|\\n|ε3(cid:105)(cid:104)ε3|\\n|ε4(cid:105)(cid:104)ε4|\\n|ε5(cid:105)(cid:104)ε5|\\n|ε6(cid:105)(cid:104)ε6|\\n\\nˆL3 ˆL2 ˆL1\\nˆL2 ˆL3 ˆL1\\nˆL3 ˆL1 ˆL2\\nˆL1 ˆL3 ˆL2\\nˆL2 ˆL1 ˆL3\\nˆL1 ˆL2 ˆL3\\n\\n105\\n6\\n10\\n7\\n10\\n105\\n\\nTable 5: Number of lines with 100% recurrence and mean recurrence, for\\ndiﬀerent initial states of the environment, the data comes from a delay em-\\nbedding for a simulation of 10000 iterations using dE = 7, δ = 0.4, and\\np = 0.8918547337153693.\\n\\n48\\n\\n\\x0c']) from 3 documents (total 3 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2743, 0.0018074814255934944),\n",
       " (3474, 0.001543951850816323),\n",
       " (1175, 0.0014621555436023581),\n",
       " (2634, 0.0013483892513330596),\n",
       " (1476, 0.0011554152087687971),\n",
       " (4259, 0.001143955640522503),\n",
       " (3630, 0.0010131970345681615),\n",
       " (2060, 0.00081790473672190066),\n",
       " (2776, 0.00075398677076950898),\n",
       " (2509, 0.0006913783044516247)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matutils.ismatrix(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yy = pyLDAvis.gensim.prepare(lda,corpus,dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_length = []\n",
    "doc_length = np.array[len(each[0]) for each in text_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[corpus]\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for doc in lda_corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hh = lda.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc in hh:\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-570af72f523a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "lda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishwoodrow/anaconda2/envs/python3/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "2017-08-21 15:42:59,615 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from sklearn import pipeline, preprocessing\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "from gensim import matutils\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def directory_list_generator(prime_directory):\n",
    "    \"\"\"Returns a list of all non-hidden directories\n",
    "    based on the path directory given, it will, return\n",
    "    only directories within the folder specified\"\"\"\n",
    "    directories=os.listdir(prime_directory)\n",
    "    dir_list = [x for x in directories if '.' not in x]\n",
    "    return dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_file_tabulator(dir_list):\n",
    "    \"\"\"Goes through all directories given as argument list\n",
    "    then picks up each text file and extracts text dumping it\n",
    "    to a column\"\"\"\n",
    "    paper_content = dict()\n",
    "    for txtDir in dir_list:\n",
    "        txtDir = prime_directory + txtDir\n",
    "        for txtfile in os.listdir(txtDir): #iterate through text files in directory\n",
    "            if txtfile[-3:] == 'txt':\n",
    "                document_path = txtDir + '/' + txtfile\n",
    "                with open(document_path) as fhand:\n",
    "                    content = fhand.read()\n",
    "                    paper_content[txtfile] = [content]\n",
    "    return paper_content              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_counter(compiled_documents):\n",
    "    \"\"\"Pipeline to convert list of documents with text\n",
    "    content from papers into a sparse matrix using count\n",
    "    vectoriser.\"\"\"\n",
    "    # Create numpy array of text data from input dictionary\n",
    "    text_data = []\n",
    "    text_data.append([v for k,v in compiled_documents.items()])\n",
    "    text_data = np.array(text_data[0])\n",
    "    # Create list of stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(['cid'])\n",
    "    # Create a CountVectorizer for parsing/counting words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=my_stop_words, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "    counts = count_vectorizer.fit_transform(text_data[:,0])\n",
    "    return counts, count_vectorizer,text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_counter_fromCSV(df):\n",
    "    \"\"\"Pipeline to convert list of documents with text\n",
    "    content from papers into a sparse matrix using count\n",
    "    vectoriser.\"\"\"\n",
    "    # Create numpy array from inputted dataframe\n",
    "    text_data = df.values\n",
    "    # Create list of stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(['cid','et','et al','al', 'yes', 'method',\n",
    "                                                   'results','citation','use','used','submitted','published'])\n",
    "    # Create a CountVectorizer for parsing/counting words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(2,3), \n",
    "                                       stop_words=my_stop_words, \n",
    "                                       token_pattern=\"\\\\b[a-z][a-z]{2,15}\\\\b\",\n",
    "                                       min_df=5,max_df=30)\n",
    "    counts = count_vectorizer.fit_transform(text_data[:,0])\n",
    "    return counts, count_vectorizer,text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_vectorizer_fromCSV(df):\n",
    "    \"\"\"Uses tfidf algorithm to return word frequency by\n",
    "    document.\"\"\"\n",
    "\n",
    "    # Create numpy array from inputted dataframe\n",
    "    text_data = df.values\n",
    "    # Create list of stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(['cid','et','et al','al', 'yes', 'method',\n",
    "                                                   'results','citation','use','used','submitted','published'])\n",
    "\n",
    "\n",
    "    # Vectorize the text using TFIDF\n",
    "    tfidf = TfidfVectorizer(ngram_range=(2,2), stop_words=my_stop_words, \n",
    "                            token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]{2,15}\\\\b\", \n",
    "                            min_df=5,max_df=30)\n",
    "    tfidf_vecs = tfidf.fit_transform(text_data[:,0])\n",
    "    return tfidf_vecs, tfidf,text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_grams_bydoc(result,model, feature_names,n_docs, n_top_words):\n",
    "    for i in range(n_docs):\n",
    "        doc_topic = np.argmax(result[i])\n",
    "        message = \"Document #%d, top topic #%d: \" % (i,doc_topic)\n",
    "        word_topic = model.components_[doc_topic]\n",
    "        message += \", \".join([feature_names[i]\n",
    "                     for i in word_topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a={'jacob':{'text': 1212,'doc_type':'paper'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b={'jake':{'text': 2323,'doc_type':'video'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c={'Tom':{'text': 4343,'doc_type':'video'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'keys' of 'dict' object needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-ae87cfa56d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'keys' of 'dict' object needs an argument"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "x = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['hw']['filetype'] =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filetype': 2}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['hw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = {**z,**c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tom': {'doc_type': 'video', 'text': 4343},\n",
       " 'jacob': {'doc_type': 'paper', 'text': 1212},\n",
       " 'jake': {'doc_type': 'video', 'text': 2323}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_csv('text_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6\\n1\\n0\\n2\\n\\n \\nr\\na\\n\\n \\n\\nM\\n3\\n2\\n\\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chapter pre-print to appear in the Oxford Hand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transferability in Machine Learning: from Phen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7\\n1\\n0\\n2\\n\\n \\nl\\nu\\nJ\\n \\n\\n6\\n1\\n\\n \\n \\n]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4\\n1\\n0\\n2\\n\\n \\nt\\nc\\nO\\n7\\n2\\n\\n \\n\\n \\n \\n]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  6\\n1\\n0\\n2\\n\\n \\nr\\na\\n\\n \\n\\nM\\n3\\n2\\n\\n \\n \\...\n",
       "1  Chapter pre-print to appear in the Oxford Hand...\n",
       "2  Transferability in Machine Learning: from Phen...\n",
       "3  7\\n1\\n0\\n2\\n\\n \\nl\\nu\\nJ\\n \\n\\n6\\n1\\n\\n \\n \\n]...\n",
       "4  4\\n1\\n0\\n2\\n\\n \\nt\\nc\\nO\\n7\\n2\\n\\n \\n\\n \\n \\n]..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vec with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm_tf, tf_vectorizer,text_data = word_counter_fromCSV(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_dtm_tf = sparse.csr_matrix(dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishwoodrow/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_topics=25, random_state=0)\n",
    "lda_fitted = lda_tf.fit_transform(sp_dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "Topic #0: scikit learn, hyper parameter, grid search, true true, learning patterns\n",
      "Topic #1: feature set, feature sets, target function, error driven, set feature\n",
      "Topic #2: sequence learning, meta learning, internal representation, representation space, universal learning\n",
      "Topic #3: text categorization, adversarial examples, train test, naive bayesian, adversarial training\n",
      "Topic #4: curriculum learning, meta learning, structured prediction, cost sensitive, meta data\n",
      "Topic #5: evolutionary computation, learning path, domain adaptation, activation functions, noise level\n",
      "Topic #6: multi label, new classes, extreme learning, new class, extreme learning machine\n",
      "Topic #7: social network, social networks, confounding factors, action recognition, convolutional layer\n",
      "Topic #8: scikit learn, low level, https www, metric learning, main features\n",
      "Topic #9: quantum machine, quantum machine learning, phys rev, learning quantum, quantum computer\n",
      "Topic #10: learning rule, hidden units, perceptron learning, storage capacity, ensemble learning\n",
      "Topic #11: arxiv prints, deep reinforcement, deep reinforcement learning, value function, dictionary learning\n",
      "Topic #12: neural machine, neural machine translation, beam search, translation model, policy gradient\n",
      "Topic #13: networks trained, auto encoders, corr abs, neural networks trained, incremental learning\n",
      "Topic #14: multi armed, armed bandits, log log log, convergence rate, boltzmann machine\n",
      "Topic #15: graphical models, exponential family, gaussian process, marginal likelihood, low rank\n",
      "Topic #16: multi task, metric learning, multi task learning, multitask learning, low rank\n",
      "Topic #17: empirical risk, matrix factorization, function class, sparse coding, max min\n",
      "Topic #18: learning agents, state action, learning agent, reinforcement learning agents, value function\n",
      "Topic #19: data representation, learning rates, matrix factorization, metric learning, bias term\n",
      "Topic #20: phys rev, deep models, generative adversarial, signal noise, learning generate\n",
      "Topic #21: human involvement, average number, given query, game playing, using reinforcement\n",
      "Topic #22: rule based, based machine, statistical machine, statistical machine translation, pos tagging\n",
      "Topic #23: sequential learning, human machine, utility function, machine human, memory learning\n",
      "Topic #24: topic model, quantum algorithm, document topic, topic models, time complexity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "countvec_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda_tf, countvec_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #0, top topic #0: scikit learn, hyper parameter\n",
      "Document #1, top topic #9: quantum machine, quantum machine learning\n",
      "Document #2, top topic #16: multi task, metric learning\n",
      "Document #3, top topic #5: evolutionary computation, learning path\n",
      "Document #4, top topic #22: rule based, based machine\n",
      "Document #5, top topic #15: graphical models, exponential family\n",
      "Document #6, top topic #0: scikit learn, hyper parameter\n",
      "Document #7, top topic #17: empirical risk, matrix factorization\n",
      "Document #8, top topic #17: empirical risk, matrix factorization\n",
      "Document #9, top topic #10: learning rule, hidden units\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm_tf, tf_vectorizer,text_data = tfidf_vectorizer_fromCSV(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_dtm_tf = sparse.csr_matrix(dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishwoodrow/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "lda_tf = LatentDirichletAllocation(n_topics=25, random_state=0)\n",
    "lda_fitted = lda_tf.fit_transform(sp_dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "Topic #0: knowledge base, epoch epoch, random samples, shalev shwartz, new classes\n",
      "Topic #1: dictionary learning, sparse coding, multi label, tree structured, scikit learn\n",
      "Topic #2: contextual bandit, learning rule, agent environment, natural images, perceptron learning\n",
      "Topic #3: mutual information, feature sets, learning rates, learning extract, feature set\n",
      "Topic #4: contextual features, context sensitive, primary features, curriculum learning, contextual information\n",
      "Topic #5: phys rev, quantum machine, rev lett, quantum learning, quantum algorithm\n",
      "Topic #6: learning patterns, feature map, marginal likelihood, feature maps, visual tracking\n",
      "Topic #7: tensor factorization, amino acid, feature engineering, feature matrix, parameter server\n",
      "Topic #8: feedback signals, boltzmann machine, hidden units, ensemble learning, target concept\n",
      "Topic #9: materials science, mit edu, students learning, hidden neurons, naive bayesian\n",
      "Topic #10: liu wang, risk minimization, sequence learning, new classes, mini batch\n",
      "Topic #11: feature set, empirical risk, learning pipeline, extreme learning, auto encoders\n",
      "Topic #12: storage capacity, learning module, level learning, convergence rate, training corpus\n",
      "Topic #13: turing machines, sequential learning, learning path, turing machine, regularization parameters\n",
      "Topic #14: theorem proving, adaptive learning, example problem, data sample, structured prediction\n",
      "Topic #15: confusion matrix, grid search, deep networks, evolutionary computation, spatial temporal\n",
      "Topic #16: multi task, metric learning, matrix factorization, decision support, multitask learning\n",
      "Topic #17: train test, hyper parameter, pos tags, recommender systems, adversarial samples\n",
      "Topic #18: neural machine, strongly convex, critical point, critical points, number nodes\n",
      "Topic #19: scikit learn, cost cost, ensemble learning, maximum margin, margin hyperplane\n",
      "Topic #20: sliding window, incremental learning, lazy learning, noun phrase, kernel means\n",
      "Topic #21: adversarial examples, adversarial training, data types, multiple kernel, adversarial example\n",
      "Topic #22: message passing, design matrix, belief propagation, discount factor, software engineering\n",
      "Topic #23: meta learning, marginal likelihood, student learning, meta features, meta data\n",
      "Topic #24: social network, given query, matrix factorization, training points, data sequence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "countvec_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda_tf, countvec_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #0, top topic #22: message passing, design matrix\n",
      "Document #1, top topic #15: confusion matrix, grid search\n",
      "Document #2, top topic #17: train test, hyper parameter\n",
      "Document #3, top topic #24: social network, given query\n",
      "Document #4, top topic #22: message passing, design matrix\n",
      "Document #5, top topic #5: phys rev, quantum machine\n",
      "Document #6, top topic #5: phys rev, quantum machine\n",
      "Document #7, top topic #3: mutual information, feature sets\n",
      "Document #8, top topic #4: contextual features, context sensitive\n",
      "Document #9, top topic #15: confusion matrix, grid search\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_grams_bydoc(lda_fitted,lda_tf, countvec_feature_names, 10,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_plot = pyLDAvis.sklearn.prepare(lda_tf, sp_dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDA_plot,'lda_features_anlysis_20features.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_jobs=1, n_topics=25, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "import csv\n",
    "import wget\n",
    "\n",
    "#converts pdf, returns its text content as a string\n",
    "def convert(fname, pages=None):\n",
    "    \"\"\"Function takes a pdf as input and converts a\n",
    "    single pdf to a text file\"\"\"\n",
    "    if not pages:\n",
    "        \n",
    "        \n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(fname, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertMultiple(pdfDir, txtDir):\n",
    "    \"\"\"converts all pdfs in directory pdfDir, \n",
    "    saves all resulting txt files to txtdir\"\"\"\n",
    "    lst_errors = []\n",
    "    # Existing conversions\n",
    "    existing_files = os.listdir(txtDir)\n",
    "    if pdfDir == \"\": pdfDir = os.getcwd() + \"\\\\\" #if no pdfDir passed in \n",
    "    for pdf in os.listdir(pdfDir): #iterate through pdfs in pdf directory\n",
    "        if not pdf+'.txt' in existing_files:\n",
    "            pdfFilename = pdfDir + pdf\n",
    "            try:\n",
    "                text = convert(pdfFilename) #get string of text content of pdf\n",
    "                textFilename = txtDir + pdf + \".txt\"\n",
    "                textFile = open(textFilename, \"w\") #make text file\n",
    "                textFile.write(text) #write text to text file\n",
    "            except:\n",
    "                print('error filename:',pdf)\n",
    "                lst_errors.append(pdf)\n",
    "                pass\n",
    "    \n",
    "    # Write any errors to a lst\n",
    "    with open('convert_errors.csv', 'w') as myfile:\n",
    "        wr = csv.writer(myfile,lineterminator='\\n')\n",
    "        for error in lst_errors:\n",
    "            wr.writerow([error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pdf_directory_convertor():\n",
    "    \"\"\"Loop through all folders containing pdfs and return\n",
    "    a text file and store it in a text folder\"\"\"\n",
    "\n",
    "    pdfDir = 'pdf/'\n",
    "    txtDir = 'txt/'\n",
    "    # Make directory if it does not exist in the text folder\n",
    "    if not os.path.exists(txtDir):\n",
    "        os.makedirs(txtDir)\n",
    "    convertMultiple(pdfDir, txtDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JLMR Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jmlr_links(primaryurl):\n",
    "    \"\"\"Gets all the links for individula papers from\n",
    "    jmlr.org\"\"\"\n",
    "    domain = 'http://www.jmlr.org'\n",
    "    full_jmlr_links = list()\n",
    "    fhand = urllib.request.urlopen(primaryurl).read()\n",
    "    soup = bs.BeautifulSoup(fhand,\"lxml\")\n",
    "    links = soup.find_all('div',{'id':'content'})\n",
    "    links_lst = links[0].find_all('a')\n",
    "    links_lst = [(primaryurl+x.get('href')) for x in links_lst[:-1]]\n",
    "    for each in links_lst:\n",
    "        fh = urllib.request.urlopen(each).read()\n",
    "        soup_paper = bs.BeautifulSoup(fh,\"lxml\")\n",
    "        papers = soup_paper.find_all('a', {'target':'_blank'})\n",
    "        paper_links_lst = [y.get('href') for y in papers]\n",
    "        for link in paper_links_lst:\n",
    "            if 'pdf' == link[-3:]:\n",
    "                if 'http' not in link:\n",
    "                    full_jmlr_links.append(domain+link)\n",
    "                else:\n",
    "                    full_jmlr_links.append(link)\n",
    "    return full_jmlr_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paper_downloader(pdf_list, output_directory):\n",
    "    \"\"\"This function downloads all the papers based on \n",
    "    a list of links sent to it\"\"\"\n",
    "    for pdf_link in pdf_list:\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "        filename = wget.download(pdf_link, out=output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1609.06935.pdf'+'.txt' in os.listdir('txt/computer_science/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', '1609.06935.pdf', '2.pdf', '3 copy.pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('pdf/computer_science/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primaryurl = 'http://www.jmlr.org/papers/'\n",
    "output_directory = 'pdf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf_list=get_jmlr_links(primaryurl)\n",
    "print('Finished updating paper list')\n",
    "paper_downloader(pdf_list, output_directory)\n",
    "print('Finished downloading paper, will start converting')\n",
    "pdf_directory_convertor()\n",
    "print('Finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
